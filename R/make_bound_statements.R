# WARNING - Generated by {fusen} from dev/flat_full.Rmd: do not edit by hand

#' converts sign constraints in the variable configuration table to rethinking::ulam ready constraints.
#' @param variable_controls Defaults to var_controls; a tibble containing start_name and varname, and sign, which is a column with constraints as '>=0' or '+'. Currently only supports >=0 or <=0, with +,- being used as alias for those.
#' @return
#' a list of boundary statements suitable for rethinking::ulam
#' 
#' @export
#' @importFrom dplyr filter
#' @importFrom dplyr select
#' @importFrom dplyr mutate
#' @importFrom dplyr across
#' @importFrom dplyr all_of
#' @importFrom dplyr arrange
#' @importFrom dplyr rowwise
#' @example
#' library(recipes)
#' library(tune)
#' library(tidyverse)
#' library(mostlytidyMMM)
#'
#' #get the control file:
#' control_file<-system.file('example model control.xlsx',package='mostlytidyMMM')
#' #get each relevant table of the control file:
#' var_controls<-readxl::read_xlsx(control_file,'variables')
#' transform_controls<-readxl::read_xlsx(control_file,'role controls')
#' workflow_controls<-readxl::read_xlsx(control_file,"workflow") |> select(-desc)
#'
#' #pull data, add fourier transform columns
#' data1=rename_columns_per_controls(read.csv(system.file("example2.csv", package = "mostlytidyMMM")),variable_controls=var_controls) |>  mutate(week=as.Date(week,"%m/%d/%Y")) |> 
#'   mutate(across(c(product),as.factor)) |> group_by(product,store) |> arrange(product,store,week) |> 
#'   mutate(day_int=as.numeric(week),
#'          cos1=cos(2*pi*day_int/356),
#'          cos2=cos(4*pi*day_int/356),
#'          cos3 =cos(6*pi*day_int/356),
#'          cos4 = cos(8*pi*day_int/356),
#'          cos5 = cos(10*pi*day_int/356),
#'          sin1=sin(2*pi*day_int/356),
#'          sin2=sin(4*pi*day_int/356),
#'          sin3=sin(6*pi*day_int/356),
#'          sin4=sin(8*pi*day_int/356),
#'          sin5=sin(10*pi*day_int/356)) |> select(-day_int) |> mutate(store=as.factor(store))
#'
#' #append several more columns
#' lotsa_vars<-paste0('x',1:60)
#' new_x<-replicate(60,runif(nrow(data1))) 
#' names(new_x)<-lotsa_vars
#' new_x<-as_tibble(new_x)
#' #bring the data together:
#' data_for_lotsa_vars<-cbind(data1 ,new_x)
#'
#' #make recipes
#' recipea<-recipe(head(data_for_lotsa_vars,n=1) )
#' recipeb<-recipea |> bulk_update_role() |> bulk_add_role()
#' recipeb<-recipeb |> add_steps_media() |>  step_select(-has_role('postprocess'))
#' recipec <-recipeb |> step_mutate(week=as.numeric(week)-19247.65) |> 
#'   update_role(c(sin1,sin2,sin3,cos1,cos2,cos3),new_role='time') |>
#'   add_role(c(sin1,sin2,sin3,cos1,cos2,cos3),new_role='predictor')
#' recipec<-recipec |> update_role(starts_with('x'),new_role='predictor')
#'
#' # build a model formula in the lmer style from the recipe:
#' (formula_with_lotsa_vars<-create_formula(recipec))
#'
#' #build a list suitable for input to rethinking::ulam from the formula and recipe:
#' (create_ulam_list(model_formula=formula_with_lotsa_vars))
#'
#'
make_bound_statements<-function(variable_controls=var_controls){
 
  bounded_coefs<-variable_controls |> filter(!is.na(sign)) |> rowwise() |> mutate(
    bound_statement =list(ifelse(sign %in% c('>=0','>0','>','+'),'lower=0',
                                 ifelse(sign %in% c('<=0','<0','<','-'),'upper=0',sign))),
    name_for_list=ifelse(tolower(role)=='outcome',varname,paste0('b_',varname))
  )
  list_of_bounds<-bounded_coefs$bound_statement
  names(list_of_bounds)<-bounded_coefs$name_for_list
  return(list_of_bounds)
}

#' Creates sampling statements for user defined priors on individual variables.
#' 
#' @param variable_controls Defaults to var_controls; a tibble containing start_name and varname, prior, and prior_sd, where the column prior is the mean of a normal dist.
#' @return
#' a list of sampling statements suitable for rehtinking::ulam()
#' 
#' @export
#' 
#' @example
make_prior_statements<-function(variable_controls=var_controls){
  prior_frame<-variable_controls |> filter(!is.na(prior)) |> select(varname,sign,prior,prior_sd) |> mutate(prior_sd=ifelse(is.na(prior_sd),prior*5,prior_sd))
  prior_frame<-prior_frame |> rowwise() |> mutate(coef_name=paste0("b_",varname),
                                                    prior_def=list(as.formula(paste0("b_",varname,"~ normal(",prior,",",prior_sd,")"))))
  
  priors_for_ulam<-prior_frame$prior_def
  names(priors_for_ulam)<-prior_frame$coef_name
  return(priors_for_ulam)
  }



#' From a combination of arguments and the user specified priors, creates the final
#' list of expressions that rethinking::ulam uses to build the stan model block.  Currently only allows additive normal model with all media transformations applied before the regression is run.  Currently only allows for random intercepts.
#' 
#' @param prior_controls Defaults to var_controls; a tibble containing start_name and varname, prior, and prior_sd, where the column prior is the mean of a normal dist.
#' @param model_formula Defaults to the object built_formula; a string containing the model formula in the style of lmer.
#' @param main_error_term_prior defaults to 'half_cauch(0,100)'; should be a sampling distribution from stan and becomes the prior on the residual error
#' @param grand_intercept_prior defaults to 'normal(50,50)'; is a string defining a sampling distribution from stan and becomes the prior on the intercept in the regression
#' @param random_int_stdev_prior defaults to 'half_cauchy(0,10)'; is a string defining the sampling distribution of the std deviation of any included random intercepts.  If there are two groups of 'random intercepts' this will apply to both.
#' @param rand_int_prior_mean defaults to 65; the mean of the prior (normal) for random intercepts. If there are two groups of 'random intercepts' this will apply to both.
#' @param unspecified_priors defaults to 'normal(0,10)'; should be a stan sampling distribution that defines the 'uninformative' prior to use when no other prior is speciied.
#' @param random_slope_stdev_prior defaults to half_cauch(0,10); should be a string describing a stan sampling distribution and is used to define the spread in any random
#' slopes in the model
#' @return
#' a list of expressions suitable for use as the formula list in rethinking::ulam
#' 
#' @export
#' 
#' @importFrom stringr::str_count
#' 
#' @example
create_ulam_list<-function(prior_controls=var_controls, model_formula=built_formula,
                           rand_int_prior_mean=65,
                           main_error_term_prior='half_cauchy(0,100)',
                           grand_intercept_prior= 'normal(50,50)',
                           random_int_stdev_prior ='half_cauchy(0,10)',
                           random_slope_stdev_prior='half_cauchy(0,10)',
                           unspecified_priors='normal(0,10)'
){
  
  #translate user defined priors
  user_defined_priors<-make_prior_statements(variable_controls = prior_controls)
  
  #create list of remaining priors -- 'uninformative' priors to be used here which is just normal(0,10)
  all_terms<-attr(terms(as.formula(model_formula)),'term.labels') 
  all_coefs<-paste0("b_",attr(terms(as.formula(model_formula)),'term.labels') )
  
  #random ints, i.e. 1|<var> we need to make the prior on <var>_int[<var>_id] and the deterministic formula will be <var>_int[<var>_id]
  random_ints0<- all_coefs[grep("b_1 | ",all_coefs,fixed=T)]
  random_ints<-gsub("^.*b_1 \\| ","",random_ints0)
  
  if(length(random_ints0)>0){
    priors_for_random_ints<-lapply(paste0(random_ints,'_int[',random_ints,'_id] ~ normal(',rand_int_prior_mean,',int_sigma)'),as.formula)
    names(priors_for_random_ints)<-random_ints
  }else{
    priors_for_random_ints=list()
  }
  #random slopes, i.e. b_var|group_var
  random_slopes0<-all_coefs[grepl("\\|",all_coefs) & !grepl("b_1 \\|",all_coefs)]
   #split into grouping vars and new 'coeff' with coeff name +_interact_<group name>
  random_slope_group_vars<-trimws(sub('.+\\|(.+)', '\\1', random_slopes0))
  random_slope_idvars<- paste0(random_slope_group_vars,"_id")
  
  random_slope_real_vars<-paste0(trimws(sub('(.+)\\|.+', '\\1', random_slopes0)),"_interact_",random_slope_group_vars)
  
  if(length(random_slopes0)>0){
    priors_for_random_slopes<-lapply(
      paste0(random_slope_real_vars,'[',random_slope_idvars,'] ~ normal(',
             0,',slope_sigma)'),as.formula)
    names(priors_for_random_slopes)<-random_slope_real_vars
  }else{
    priors_for_random_slopes=list()
  }
  #get diffuse priors on fixed effects not specified by user
 fixed_coefs<-all_coefs[!(all_coefs %in% random_ints0) & !(all_coefs %in% random_slopes0)]
  fixed_terms<-all_terms[!(all_coefs %in% random_ints0) & !(all_coefs %in% random_slopes0)]
   
  
  fixed_coefs_needing_priors<-fixed_coefs[!(fixed_coefs %in% names(user_defined_priors))]
  
  priors_for_fixed<-lapply(paste0(fixed_coefs_needing_priors,"~",unspecified_priors),as.formula)
  names(priors_for_fixed)<-fixed_coefs_needing_priors
  
  main_model_formula<-as.formula(paste0(as.character(as.formula(model_formula))[2],'~ normal(big_model,big_sigma)'))
  #prior on big_sigma
  prior_on_big_sigma<-as.formula(paste('big_sigma ~ ',main_error_term_prior))
  #prior on grand intercept
  prior_on_a0<-as.formula(paste('a0 ~ ',grand_intercept_prior))
  
  #prior_on_store_ints_spread
  prior_on_store_int_sigma<-as.formula(paste('int_sigma ~',random_int_stdev_prior))
  #prior on random slope spread
  prior_on_slope_sigma<-as.formula(paste('slope_sigma ~',random_slope_stdev_prior))
  #more than around 15 terms to sum for the expected value in the model, need to split it rethinking::ulam builds an incorrect stan file.
  
  
  number_of_terms<-length(all_terms)
  
  rand_ints_formula_for_ulam<-  ifelse(length(random_ints0)>0,
                                       paste(paste0(random_ints,"_int[",random_ints,"_id]"),collapse='+'),
                                       '')
  rand_slopes_formula_for_ulam<-ifelse(length(random_slopes0)>0,
                                       paste(paste0(random_slope_real_vars,'[',random_slope_idvars,']'),collapse=' + '),
                                       '')
  
  number_of_fixed<-number_of_terms-
    stringr::str_count(rand_slopes_formula_for_ulam,"\\[") - 
    stringr::str_count(rand_ints_formula_for_ulam,"\\[") 
  
  big_model_list<-vector('list')
  included_terms=0
  start_term=1
  last_end_term=15
  iter_of_big_model=1
  while(start_term<=number_of_fixed){
    
    this_end_term=min(last_end_term,number_of_fixed)
    if(iter_of_big_model==1){
      big_model_list[[iter_of_big_model]]<- paste(paste0('big_model_',iter_of_big_model,' <-'),paste(fixed_coefs[start_term:this_end_term],
                                                                                                     fixed_terms[start_term:this_end_term],sep='*',collapse='+'))
    }
    else{
      big_model_list[[iter_of_big_model]]<-paste(paste0('big_model_',iter_of_big_model,' <- big_model_',iter_of_big_model-1,' + '),paste(fixed_coefs[start_term:this_end_term],
                                                                                                                                         fixed_terms[start_term:this_end_term],sep='*',collapse='+'))
    }
    start_term=this_end_term+1
    last_end_term=this_end_term+15
    iter_of_big_model=iter_of_big_model+1
  }
  if(rand_ints_formula_for_ulam!='' & rand_slopes_formula_for_ulam != '')
  {big_model_list[[iter_of_big_model]]<-paste(paste0('big_model <- big_model_',
                                                     iter_of_big_model-1),'a0',
                                              rand_ints_formula_for_ulam,
                                              rand_slopes_formula_for_ulam,sep='+')}
  else if(rand_ints_formula_for_ulam!=''){
    big_model_list[[iter_of_big_model]]<-paste(paste0('big_model <- big_model_',
                                                      iter_of_big_model-1),'a0',
                                               rand_ints_formula_for_ulam,sep='+')
  }else if(rand_slopes_formula_for_ulam!=''){
    big_model_list[[iter_of_big_model]]<-paste(paste0('big_model <- big_model_',
                                                      iter_of_big_model-1),'a0',
                                               rand_slopes_formula_for_ulam,sep='+')
  }
    else
    {big_model_list[[iter_of_big_model]]<-paste(paste0('big_model <- big_model_',
                                                     iter_of_big_model-1),'a0',sep='+')}
  
  #to have expressions in the final list, need to prase the strings in big_modl
  big_model_list_parsed<-sapply(big_model_list,function(x) parse(text=x))
  
  formula_list<-c(main_model_formula,rev(big_model_list_parsed),priors_for_random_slopes,priors_for_random_ints,priors_for_fixed,user_defined_priors,prior_on_a0,prior_on_big_sigma,prior_on_store_int_sigma,prior_on_slope_sigma)
  
  class(formula_list)<-'list'
  return(formula_list)
}


#'a prediction function for ulam objects, as comes from rethinking::ulam
#'
#' @param ulamobj the out of rethinking::ulam
#' @param new_data the dataset on which to draw samples and make predictions
#' @param n the number of samples to draw
#' @param reduce defuaults to TRUE; a boolean that indicates if the samples should be returned or a mean and high credibility interval
#' @param conf defaults to .95, indicates the desired converage of credibility interval when reduce=T
#'
#' @return
#'if reduce=T a tibble with pred, pred_lower_conf, pred_upper_conf columns
#'else if reduce =F the output of link (a matrix with sampled predictions)
#'
#' @export
#' @export predict.ulam
#' @importFrom rethinking link
#' @importFrom stats predict
predict.ulam<-function(ulamobj,new_data,n=1000,reduce=T,conf=.95){
  link_output<-link(ulamobj,new_data=new_data,n=n)$big_model
  if(reduce){
    preds<-colMeans(link_output)
    low_conf<-apply(link_output,2,quantile,1-conf)
    high_conf<-apply(link_output,2,quantile,conf)
    pred_output<-cbind(preds,low_conf,high_conf)
    colnames(pred_output)<-c('pred',paste0('pred_lower_',conf),paste0('pred_upper_',conf))
  }
  return(pred_output)
}

#'pulls predictor variable names from a recipe except those that have role2 %in% c('group','time')
#'
#' @param base_recipe defaults to recipe3; is the recipe to pull predictor names from
#'
#' @export
#'
#' @importFrom dplyr filter
#' @importFrom dplyr select
#' @importFrom dplyr mutate
#' @importFrom dplyr across
#' @importFrom dplyr all_of
#' @importFrom dplyr arrange
#'
#' @return a vector of variablnames that have role == 'predictor'
#'
#' @example
get_predictors_vector<-function(base_recipe=recipe3){
  groups_and_time<-unlist(summary(base_recipe) |> filter(role %in% c('group','time')) |> select(variable))
  
  predictors<-unlist(summary(base_recipe) |> 
                       filter(role=='predictor') |> select(variable))
  names(predictors)<-NULL
  return(predictors[!(predictors %in% groups_and_time)])
}

#'pulls vector of identification variables from recipe using role== group or role==time_id
#' @param base_recipe defaults to recipe3; a recipe produced in the mostlytidyMMM format
#' @return
#' a character vector
#' @example 
#' @export
#' @importFrom dplyr filter
#' @importFrom dplyr select
#' @importFrom dplyr mutate
#' @importFrom dplyr across
#' @importFrom dplyr all_of
#' @importFrom dplyr arrange
get_id_vector<-function(base_recipe=recipe3){
  groups_and_time<-unlist(summary(base_recipe) |> filter(role %in% c('group','time_id')) |> select(variable))
  names(groups_and_time)<-NULL
    return(groups_and_time)
}


#'pulls outcome variable from recipe using role== group or role==time_id
#' @param base_recipe defaults to recipe3; a recipe produced in the mostlytidyMMM format
#' @return
#' a character vector
#' @example 
#' @export
#' @importFrom dplyr filter
#' @importFrom dplyr select
#' @importFrom dplyr mutate
#' @importFrom dplyr across
#' @importFrom dplyr all_of
#' @importFrom dplyr arrange
get_outcome_vector<-function(base_recipe=recipe3){
  groups_and_time<-unlist(summary(base_recipe) |> filter(role %in% c('outcome')) |> select(variable))
  names(groups_and_time)<-NULL
  return(groups_and_time)
}
#' Pulls values from the control tibble by name; convenience function for working with the control tibble.
#' 
#' @param this_control is the string containing the control of interest
#' @param control defaults to workflow_controls; is a tibble containing columns R_name and Value, 
#' @return
#' The value of control$Value when control$R_name==this_control; typically will be a character, frequently requires coercion to other types.
#' @export
#' @importFrom dplyr filter
get_control<-function(this_control,control=workflow_controls){
  if(length(this_control)==0){stop("get_control requires this_control to be non-null")}
  control |> filter(R_name==!!this_control) |> select(Value) |> unlist()
}


#' Creates a string that represents a model formula from a recipe and the workflow controls data
#' 
#' @param base_recipe defaults to recipe3; is the recipe containing variables to build a formula for
#' @param control defaults to workflow_controls, should be a tibble with columns R_name and Value, which must have rows with R_name =='Y','list_rand_ints' and 'fft_terms'
#' @param ignore_rands defaults to FALSE, but if set to TRUE the created formula 
#' will not contain random intercepts or slopes other than fourier transform 
#' related slopes (if those are specified in the control frame)
#' @return
#' a string that reads like an lmer formula
#' @export
#' 
#' @example
create_formula<-function(base_recipe=recipe3,control=workflow_controls,ignore_rands=FALSE){
  #we will remove grouping vars (used for random effects) and time series stuff (for now)
  
  final_predictors<-get_predictors_vector(base_recipe=base_recipe)
  outcome<-get_control(this_control='Y',control=control)  
  fft_interaction_term<-get_control("interaction_fft",control=control)
  
  fft_terms<-get_control("fft_terms",control=control) |> as.numeric()
  fft_terms<-ifelse(is.na(fft_terms),0,fft_terms)
  
  fft_formula<-""
  
  if(fft_terms>0){
    for(i in 1:fft_terms){
      if (i==1){
        if(fft_interaction_term!="" & !is.na(fft_interaction_term)){ fft_formula=
            paste0("(sin1|",fft_interaction_term,") + (cos1|",fft_interaction_term,")")}
        else{  fft_formula= 'sin1 + cos1'}
      }
      else{
        if(fft_interaction_term!="" & !is.na(fft_interaction_term)){
          new_term<-paste(paste0('(',c('cos','sin'),i,'|',fft_interaction_term,')'),sep='',collapse='+')
          fft_formula=paste(fft_formula,new_term,sep='+')
        }
        else{
        fft_formula=paste(fft_formula,paste(c('cos','sin'),i,sep='',collapse='+'),sep='+')
        }
      }}}
  
  list_rand_ints<-gsub(" ","",get_control("list_rand_ints",control=control),fixed=T) |> strsplit(',',fixed=T) |> unlist()
  list_rand_ints<-list_rand_ints[!is.na(list_rand_ints)]
  
  rand_int_formula<-""
  if(length(list_rand_ints)>0){
    rand_int_formula<-paste(list_rand_ints,collapse =' + ')
  }
  
  list_rand_slopes<-gsub(" ","",get_control("list_rand_slopes",control=control),fixed=T) |> strsplit(',',fixed=T) |> unlist()
  list_rand_slopes<-list_rand_slopes[!is.na(list_rand_slopes)]
  
  rand_slope_formula<-""
  if(length(list_rand_slopes)>0){
    rand_slope_formula<-paste(list_rand_slopes,collapse =' + ')
  }
  
  if(!ignore_rands){
   built_formula<-paste(paste0(outcome,' ~ ',
                              paste(final_predictors,collapse=' + ')),
                       fft_formula,rand_int_formula,rand_slope_formula,sep=' + ')
  }
  if(ignore_rands){
    built_formula<-paste(paste0(outcome,' ~ ',
                                paste(final_predictors,collapse=' + ')),
                         fft_formula,sep=' + ')
  }
  #remove trailing +, left by the paste in built_formula if one of fft_formula, 
  #rand_int_formula, or rand_slope_formula are empty strings
  total_times<-(length(list_rand_slopes)==0)+(length(list_rand_ints)==0)+(fft_terms==0)
  
  if (total_times>0){
    for(i in 1:total_times){
      built_formula<-gsub("(\\+\\s* $)","",built_formula)
    }
    built_formula<-trimws(built_formula)
  
  }
  built_formula <- gsub("\\+  \\+", "\\+", built_formula)  
  return(built_formula)
}


#' produce a list of  formula for testing combinations of random effects
#' 
#' @param vc defaults to best_seas vc; a tibble with a single seasonality specification typically determined
#' by the seasonality_search process
#' @param seasonality_formula defaults to best_seas_formula; a string containing the formula for
#' all fixed effects in the model plus the seasonaity terms (which may include random slopes).
#' Typically will be produced by tuning a workflowset to identify the best seasonality specification.
#' @export
#' @return a list of strings suitable to be coerced to formula for lmer
#' and the specifications for random effects.  In the case where the control
#' search_randoms = 'FALSE' , the string in seasonality_formula is returned in a list of 1
#' 
make_list_of_rands_formula<-function(seasonality_formula=best_seas_formula,
                                     vc=best_seas_vc){
  
  if(get_control("search_randoms",vc)=="FALSE"){
    #in this case we expect that the seasonality_formula will already contain
    # any random effects specified (unless some process has problematically tampered
    #with the search_randoms control)
    
    random_terms=gsub(',','+',paste(get_control("list_rand_ints",vc),
                                    get_control('list_rand_slopes',vc),sep=','),fixed=T)
    
    list_of_formulae_rands<-list(seasonality_formula)
    cat("Nota Bene: make_list_of_rands_formula has been called 
        when search_randoms is FALSE\n Output is a single formula string matchting seasonality_formula.")
  }
  
  else{
    list_of_formulae_rands<-vector('list')
    #get all possible terms
    terms_to_add<-paste(get_control("list_rand_ints",best_vc),
                        get_control('list_rand_slopes',best_vc),sep=',') %>% strsplit(split=',') %>% unlist()
    #create all_combinations using collapsed combn output
    pastey<-function(x){paste(unlist(x),collapse=" + ")}
    all_combinations<-unlist(lapply(1:length(terms_to_add),function(x) combn(terms_to_add,x,FUN=pastey,simplify = F)))
    
    for(i in 1:length(all_combinations)){
      list_of_formulae_rands[[i]]<-paste0(seasonality_formula,'+',all_combinations[[i]])
    }
    list_of_formulae_rands<-append(list_of_formulae_rands,seasonality_formula)
  }
  return(list_of_formulae_rands)
}

#' creates a tidymodels workflow from a a recipe and a model
#' @param this_formula defaults to built_formula; a string suitable to be coerced to a formula for lmer
#' @param recipe_to_use defaults to recipe3; a recipe with roles for predictor, outcome, time_id, and groups
#' @export
#' @example 
#' @importFrom workflows workflow
assemble_workflow<-function(this_formula=built_formula,recipe_to_use=recipe3){
  if(grepl("\\|",this_formula)){tune_spec<-linear_reg(engine='lmer')} else{
    tune_spec<-linear_reg(engine='lm')}
  #  hardhat::extract_parameter_set_dials(reg_wf)|> finalize(data1) 
  #need to build a formula with random effects specs for stan_glmer
  
  return(workflow() |>  add_recipe(recipe_to_use) |> 
           add_model(tune_spec,formula=as.formula(this_formula)))
}

#' create a list of model formula (per the rules for lmer()) with different
#' seasonality specifications as listed in the configuration tibble.
#' 
#' @param vc defaults to workflow_controls; a tibble with R_name and Value columns
#' where R_name contains the control name and Value is it's value
#' @param recipe_to_use defaults to recipe3; a recipes::recipe with pre-modeling transformations and
#' variable roles; the outcome, predictor, group, and time_id roles are used in determining
#' the formula.
#' @return a list with names formulae and configs, formulae is a list of strings suitable
#'  to be coerced to a formula and passed to lmer.  configs is a list of tibbles with information like vc,
#'   but reset to match a single seasonality specification.  If search_seasonality is set to FALSE in
#'   the config table, the two lists have a single formula containing the specified seasonality
#'   and no random terms.  A note is printed to the console in this case.
#' @example 
#' @export
#' @importFrom 
make_list_of_fft_formulae<-function(vc=workflow_controls,recipe_to_use=recipe3){
  #use search_randoms control value to determine if formula created here will include
  #random effects not in seaonslity. If search_randoms if TRUE, we do want to ignore
  #random effects specification (for later searching)
  are_we_ignoring_rands = get_control('search_randoms')=="TRUE"
  if(get_control("search_seasonality")=="FALSE"){
    list_of_configs=list(vc)
    formulae=list(create_formula(recipe_to_use,vc,ignore_rands=are_we_ignoring_rands))
    cat("Nota Bene: make_list_off_formulae is called but search_seaonality is FALSE")
  }else{
    fft_interact_options<-get_control("interaction_fft") |> strsplit(split=',',fixed=T) |> unlist()
    if(!("" %in% fft_interact_options)){fft_interact_options=c("",fft_interact_options)}
    
    fft_count_options<-get_control("fft_terms") |> unlist() |> as.numeric()
    #vc<-workflow_controls
    list_of_configs<-vector('list')
    idx=0
    for (this_fterms in 0:fft_count_options){
      for (iterm in 1:length(fft_interact_options)){
        idx=idx+1
        this_iterm=fft_interact_options[iterm]
        
        this_vc=vc |> mutate(Value=ifelse(R_name=='interaction_fft',this_iterm,Value),
                             Value=ifelse(R_name=='fft_terms',this_fterms,Value),
                             Value=ifelse(R_name=='search_seasonality',"FALSE",Value))
        list_of_configs[[idx]]=this_vc
      }
    }
  }
  formulae<-lapply(list_of_configs,function(x) create_formula(recipe_to_use,x,ignore_rands = are_we_ignoring_rands))
  return(list(formulae=formulae,configs=list_of_configs))
}


#' get decomps agnostic to multiplicative or additive model specification
#'
#' @param data_to_use defaults to data 3; a transformed dataset (likely produced via bake())
#' @param recipe_to_use defaults to recipe3; a recipe which has the roles assigned for the MMM
#' @param model_obj defaults to rethinking_results; an ulam fit object
#' @param predictors defaulst to get_predictors(recipe3)
#' @param sample_size seems to be useless input to link() function
#' @export
#' @importFrom rethinking link
#' @example
#' @return
#' a tibble with one row per row in data_to_use, incluiding id columns as pulled from
#' the recipe_to_use (ie where role == group or role==time_id), and the one column
#' per predictor variable that shows the amount of the outcome variable due to that predictor.
#' 
#' Additionally, the columns decomp_base, decomp0_tot, pred, and decomp_ratio are appended.
#' 
#' decomp_base is the amount of the outcome variable due to intercept(s) and other no response
#' function needed variables.  decomp0_tot is the sum of the predictor var decomp columns prior
#' to rescaling.  decomp_ratio is the ratio of pred (the full predicted outcome) to decomp0_tot.
#' This has been applied as scaling factor -- it will generally be different from 1 for multiplicative models
#' and be 1 for additive models.
#' @details
#' The decomp algorithm works by first setting all predictors that are not _id variables (as found
#' by get_predictors_vector() to zero and calling link() to determine the prediction for this scenario.
#' 
#' That prediction becomes the initial decomp_base.
#' 
#' Then each predictor is set back to historical values one at a time, and the initial decomp for that
#' variable is set to the difference in prediction with that predictor 'on' and the decomp_base.
#' 
#' Finally, the sum of all the initial decomps is taken, and the ratio between the pred and the sum of decomps
#' is used to scale all decomps (so the sum is equal to prediction).
#' 
#' For additive models, this is equivalent to decomp = model_coef * modeled_independent_variable; for
#' multiplicative models this is equivalent to thinking of decomps as lift over base, with an adjustment to make it
#' additive.
get_decomps_irregardless<-function(data_to_use=data3,recipe_to_use=recipe_finalized,model_obj=rethinking_results,
                             predictors=get_predictors_vector(recipe_to_use),sample_size=1000){
  
  
  #predictors will need to be trimmed of *_id variables (at least until we know how to predict with missinf id values)
  predictors<-predictors[!grepl("_id",predictors,fixed=T)]
  
  variations<-vector('list',length=length(predictors))
  predictors_to_zero<-data_to_use |>
    mutate(across(all_of(!!predictors),function(x) 0 ))
  for(i in 1:length(variations)){
    variations[[i]]<-predictors_to_zero
    variations[[i]][predictors[i]]<-data_to_use[predictors[i]]
  }
  names(variations)<-predictors
  
  decomp_names<-c(names(variations),"decomp_base")
  preds_variations<-lapply(variations,function(x) {colMeans(link(model_obj,x,n=sample_size)$big_model)})
  
  preds_all<-colMeans(link(model_obj,data_to_use,n=sample_size)$big_model)
  
  #intercept_only
 
  decomp_base<-colMeans(link(model_obj,
                                predictors_to_zero,
                                n=sample_size
                                )$big_model)
  
  #this initial delta will be not additive if model is mutiplicative
  list_decomps_0<-lapply(preds_variations,function(x) x-decomp_base)
  decomps_0<-as_tibble(list_decomps_0) 
  #sum decomps_0 . . .
  # decomps_0$int_only<-int_off_preds-preds_main
  
  decomps_0$decomp_base=decomp_base
  decomps_0$decomp0_tot<-rowSums(decomps_0,na.rm=T)
  decomps_0$pred<-preds_all
  decomps_0$decomp_ratio <-  decomps_0$pred/decomps_0$decomp0_tot
  decomps_1<- decomps_0 |>mutate(across(all_of(!!decomp_names),function(x) x*decomp_ratio))
  #to make sure output is clear, need to rename these to decomps_
  #append the columns in data_to_use that are not in decomps_1

  decomps_1<-cbind(data_to_use %>% select(all_of(get_id_vector(recipe_to_use))),
                   data_to_use %>% select(all_of(get_outcome_vector(recipe_to_use))),
        decomps_1)
  return(decomps_1)
}


update_range_from_control<-function(parameter_set,controls){
  for(i in 1:length(parameter_set$id)){
    parameter_set$object[i][[1]]<-do.call(
      unlist(controls[controls$dial_id==parameter_set$id[i],'dial_func']),
      list(range=unlist(controls[controls$dial_id==parameter_set$id[i],'new_range']))
    )
  }
  return(parameter_set)
}

create_dials_from_wf_and_controls<-function(workflow=mmm_wf,control_ranges=transform_controls){
  tune_these_parms<-extract_parameter_set_dials(workflow) #will have default ranges
  if(nrow(tune_these_parms)==0){return(tune_these_parms)}
  #get ranges from transform_controls
  gamma_ranges<-control_ranges %>%rowwise() %>%  mutate(dial_id=paste0(role,"_saturation_speed")) %>% 
    mutate(new_range=map2(list(saturation_speed_low),list(saturation_speed_high),~c(.x,.y)),
           dial_func='saturation_speed') %>% select(dial_id,new_range,dial_func)
  
  alpha_ranges<-control_ranges %>%rowwise() %>%  mutate(dial_id=paste0(role,"_asymptote")) %>% 
    mutate(new_range=map2(list(asymptote_low),list(asymptote_high),~c(.x,.y)),dial_func='asymptote') %>% select(dial_id,new_range,dial_func)
  
  retention_ranges<-control_ranges %>%rowwise() %>%  mutate(dial_id=paste0(role,"_retention")) %>% 
    mutate(new_range=map2(list(retention_low),list(retention_high),~c(.x,.y)),dial_func='retention') %>% select(dial_id,new_range,dial_func)
  
  
  tune_these_parms<-update_range_from_control(tune_these_parms,rbind(gamma_ranges,alpha_ranges,retention_ranges))
  return(tune_these_parms)
}
