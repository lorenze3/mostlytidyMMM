---
title: "flat_rmd for mostlytidyMMM"
output: html_document
editor_options: 
  chunk_output_type: console
---

<!-- Run this 'development' chunk -->
<!-- Store every call to library() that you need to explore your functions -->

```{r development, include=FALSE}
library(testthat)
library(tidymodels)
library(recipes)
library(rethinking)
library(workflowsets)
library(tune)
library(tidyverse)
```



```{r development-2,include=FALSE}
# Run all this chunk in the console directly
# Create "inst/" directory
# dir.create(here::here("inst"))
# 
# # Example dataset
# file.copy(system.file("nyc_squirrels_sample.csv", package = "fusen"), here::here("inst"))
# # Make your dataset file available to the current Rmd
# pkgload::load_all(path = here::here(), export_all = FALSE)
# 
# # You will be able to read your example data file in each of your function examples and tests as follows
# datafile <- system.file("nyc_squirrels_sample.csv", package = "mostlytidyMMM")
# nyc_squirrels <- read.csv(datafile)
# 
# nyc_squirrels
```


# Custom recipe steps 

These functions define adstock and saturation functions ($ f(x) = A*(1-e^{-k*x} $) for use in recipes (ie tidymodels pre-model transformation pipelines).

```{r function-step-sat-1}
# #' working around some fusen issue; re-exporting bake to this namespace
# #' @export
# bake<-recipes::bake
# 
# #' working around some fusen issue; re-exporting bake to this namespace
# #' @export
# prep<-recipes::prep
# 
# #' working around some fusen issue; re-exporting bake to this namespace
# #' @export
# tunable<-tune::tunable


#' function to add saturation trasnformations to a recipe
#'
#' @param recipe a recipes::recipe
#' @param ... variables in recipe to be effected by the step
#' @param role defaults to NA, set to a string to change the role of the transformed variables
#' @param asymptote defaults to 100, the asymptotic value of the saturation transformed variable
#' @param saturation_speed defaults to .05, controls how quickly the asymptote is approached
#' @param id default value will work most often, but can be set manually for use with tuning fuctions
#'
#' @return
#' a recipe with the new saturation step appended
#' @export
#'
#' @examples
step_saturation <- function(
    recipe, 
    ..., 
    role = NA, 
    trained = FALSE, 
    options = list( names = TRUE),
    skip = FALSE,
    asymptote=100,
    saturation_speed=.05,
    hills=NULL,
    id = rand_id("saturation")
) {
  
  add_step(
    recipe, 
    step_saturation_new(
      terms = enquos(...), 
      trained = trained,
      role = role, 
      options = options,
      skip = skip,
      id = id,
      asymptote=asymptote,
      saturation_speed=saturation_speed,
      hills=hills
    )
  )
}

#' internal function used in create recipe steps
#'
#'
#' @return
#' a recipe sep of type saturation
#' @export
step_saturation_new <- 
  function(terms, role, trained, asymptote,saturation_speed,hills, options, skip, id) {
    step(
      subclass = "saturation", 
      terms = terms,
      role = role,
      trained = trained,
      asymptote=asymptote,
      saturation_speed=saturation_speed,
      hills=hills,
      options = options,
      skip = skip,
      id = id
    )
  }

#' a monotonic saturation function with monotonic first deriviate used in recipes for preprocssing MMM data
#' 
#' @param x vector of numeric value to saturate
#' @param asymptote the asymptote of the transformed values; should probably exceed the max value in the modeled data
#' @param saturation_speed higher values approach the asymptote faster
#'
#'
#' @return
#' a numeric vector of transformed values
#' @export
#worker bee function
get_train_saturation<-function(x,asymptote,saturation_speed){
  stopifnot(length(asymptote) == 1)
  stopifnot(length(saturation_speed) == 1)
  x_scurve<-asymptote*(1-exp(-saturation_speed*x))
  
  return(x_scurve)
}

#' The saturation step prep method
#' 
#' @param x an object (step or recipe)
#' @param training  tibble used to provide data for training
#' 
#' @return
#' a prepped recipe or step
#' @export
#' @export prep.step_saturation
#' 
#' @importFrom recipes prep
#' 
prep.step_saturation <- function(x, training, info = NULL, ...) {
  
  #this will select the appropriate columns from the training set
  col_names <- recipes_eval_select(x$terms, training, info) 
  
  ## We'll use the names later so make sure they are available
  if (x$options$names == FALSE) {
    rlang::abort("`names` should be set to TRUE")
  }
  
  #transforms computed here
  step_saturation_new(terms=x$terms,
                      trained=TRUE,
                      role=x$role,
                      options=x$options,
                      skip=x$skip,
                      id=x$id,
                      asymptote=x$asymptote,
                      saturation_speed=x$saturation_speed,
                      hills=col_names
  )
  
}

#'The saturation bake method
#'
#'@param object recipe or step
#'@param mdata the data fed to the recipe or step
#'
#'@return
#'a transformed dataset
#'
#'@export
#'@export bake.step_saturation
#'
#'@importFrom recipes bake
#'@importFrom dplyr group_by
bake.step_saturation<-function(object,new_data,...){
  vars<-names(object$hills)
  groupings<-as.character(groups(new_data))
  
  new_data[,vars]<-new_data[,object$hills] |> reframe(across(everything(),function(x){get_train_saturation(x,
                                                                                                            object$asymptote,object$saturation_speed)})) 
  if(length(groupings)>0) {new_data<-as_tibble(new_data) |> dplyr::group_by(across(all_of(groupings)))}
  else{new_data<-as_tibble(new_data)}
  return(new_data)
  
}

#' print method for saturation steps
#' 
#' @param x step
#' @return
#' string
#' @export
#' @export print.step_saturation
#' 
print.step_saturation <-
  function(x, width = max(20, options()$width - 35), ...) {
    
    print_step(
      # Names before prep (could be selectors)
      untr_obj = x$terms,
      # Names after prep:
      tr_obj = names(x$hills),
      # Has it been prepped? 
      trained = x$trained,
      # An estimate of how many characters to print on a line: 
      width = width,
      title = paste("Saturation (asymptote=",x$asymptote,"saturation_speed=",x$saturation_speed,"Transformation on")
    )
    invisible(x)
  }

#'tunable method for saturation steps -- used by tune package to identify what each hyperparameter is.
#'
#'@param x step or recipe
#'@return
#'tibble that tune functions can evaluate as hyperparameter information
#'@export
#'@export tunable.step_saturation
#'
#'@importFrom tune tunable
tunable.step_saturation <- function (x, ...) {
  tibble::tibble(
    name = c("asymptote","saturation_speed"),
    call_info = list(list( fun = "asymptote"),list(fun='saturation_speed')),
    source = c("recipe","recipe"),
    component = c("step_saturation","step_saturation"),
    component_id = x$id
  )
}

#' dial (ie from the tune package) for asymptote
#' 
#' @param range defaults to c(100,300), should be a length2 numeric vector that has the minimum and maximum possible asymptotes for a given variable transformation.  In general the asymptote should be higher than the max value in the historical data.
#' @return
#' a quant param dial
#' @export
#' @importFrom dials new_quant_param
asymptote<-function(range=c(100,300)){dials::new_quant_param(type='double',range=range,inclusive=c(FALSE,TRUE),
                                                    label=c(asymptote='saturation asymptote'),finalize = NULL)}

#' dial (ie from the tune package) for saturation_speed
#' 
#' @param range defaults to c(.0001,.009), should be a length 2 numeric vector that has the minimum and maximum possible saturation speed. Small changes in this value have large impact of rate of saturation.
#' @return
#' a quant param dial
#' @export
#' @importFrom dials new_quant_param
saturation_speed<-function(range=c(.0001,.009)){dials::new_quant_param(type='double',range=range,inclusive=c(FALSE,FALSE),                                                            label=c(saturation_speed='saturation speed'),finalize = NULL)}

```


<!--
Create a chunk with an example of use for your function

- The chunk needs to be named `examples` at least
- It contains working examples of your function
- The chunk is better be named `examples-my_median` to be handled
correctly when inflated as a vignette

After inflating the template

-  This example will be added in the '@examples' part of our function above in the "R/" directory
- This example will be added in the vignette created from this Rmd template
-->

```{r examples-saturation}
#not run!
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(recipes)))
suppressMessages(suppressWarnings(library(tune)))
suppressMessages(suppressWarnings(library(dials)))
suppressMessages(suppressWarnings(library(mostlytidyMMM)))

#create two datasets
mktdata<-rbind(tibble(prod='brand',store='store1',
                      sales=c(100,100,100,100,100),tv=c(10,100,0,0,100),
                      search=c(0,10,20,50,50)) ,
               tibble(prod='brand',store='store2',
                      sales=c(10,10,10,10,10),tv=c(0,0,0,0,0),
                      search=c(0,2,2,0,0) ) ) |> 
  group_by(prod,store)

mktdata2<-tibble(prod='brand',store='all',
                 sales=100,tv=1000,search=1000) |> group_by(prod,store)

#creating a recipe with a saturaiton step in it
rec_obj <-
 recipe(sales ~ ., data = mktdata) |>
 step_saturation(c(tv,search),asymptote=200,saturation_speed=.003 ) |>
 prep(training = mktdata)

#showing what a tunable recipe looks liek:
rec_obj2<-  recipe(sales ~ ., data = mktdata) |>
  step_saturation(c(tv,search),asymptote=tune(),saturation_speed=tune() )

extract_parameter_set_dials(rec_obj2)

 #custom print shows values of saturation hyperparameters:
 print(rec_obj)
 #or shows that they are set to be tuned:
 print(rec_obj2)
 #producing the transformed dataset:
 bake(rec_obj,mktdata)

```

<!--
Create a chunk with a test of use for your function

- The chunk needs to be named `tests` at least
- It contains working tests of your function
- The chunk is better be named `tests-my_median` to be handled
correctly when inflated as a vignette

After inflating the template

-  This test code will be added in the "tests/testthat/" directory
-->

```{r tests-saturation}

test_that("saturation steps peform as expected in a recipe", {
  library(tune)
  library(recipes)
  library(tidyverse)
  library(dials)
  mktdata<-rbind(tibble(prod='brand',store='store1',sales=c(100,100,100,100,100),tv=c(10,100,0,0,100),search=c(0,10,20,50,50)) ,
               tibble(prod='brand',store='store2',sales=c(10,10,10,10,10),tv=c(0,0,0,0,0),search=c(0,2,2,0,0) ) ) |> group_by(prod,store)

mktdata2<-tibble(prod='brand',store='all',sales=100,tv=1000,search=1000) |> group_by(prod,store)

rec_obj <-
 recipe(sales ~ ., data = mktdata) |>
 step_saturation(c(tv,search),asymptote=200,saturation_speed=.003 ) |>
 prep(training = mktdata)

rec_obj2<-  recipe(sales ~ ., data = mktdata) |>
  step_saturation(c(tv,search),asymptote=tune(),saturation_speed=tune() )

true_mktdata_baked<-readRDS(system.file("baked_mktdata_sat_only.RDS", package = "mostlytidyMMM"))
true_mktdata2_baked<-readRDS(system.file("baked_mktdata2_sat_only.RDS", package = "mostlytidyMMM"))
 true_tunable<-readRDS(system.file("true_tunable_saturation.RDS", package = "mostlytidyMMM"))
 # expect_true(identical(extract_parameter_set_dials(rec_obj2),true_tunable))
  expect_true( identical(recipes::bake(rec_obj,mktdata),true_mktdata_baked))
  expect_true( identical( recipes::bake(rec_obj,mktdata2),true_mktdata2_baked))
  
})
```


```{r function-adstock-step-1}
#' function to add saturation trasnformations to a recipe
#'
#' @param recipe a recipes::recipe
#' @param ... variables in recipe to be effected by the step
#' @param role defaults to NA, set to a string to change the role of the transformed variables
#' @param retention defaults to .5, the amount of impact retained from the previous time period
#' @param id default value will work most often, but can be set manually for use with tuning fuctions
#'
#' @return
#' a recipe with the new adstock step appended
#' @export
#'
#' @examples
#' 
step_adstock <- function(
    recipe, 
    ..., 
    role = NA, 
    trained = FALSE, 
    options = list( names = TRUE), #change to be range of retention
    skip = FALSE,
    retention=.5,
    groups=c('prod','store'),
    time_id='week',
    adstocks=NULL,
    id = rand_id("adstock")
) {
  
  add_step(
    recipe, 
    step_adstock_new(
      terms = enquos(...), 
      trained = trained,
      role = role, 
      options = options,
      skip = skip,
      id = id,
      retention=retention,
      adstocks=adstocks,
      groups=groups,
      time_id=time_id
    )
  )
}

#' internal function used in create recipe steps
#'
#'
#' @return
#' a recipe sep of type saturation
#' @export
#' 
#' @example
#' 
step_adstock_new <-   function(terms, role, trained, retention, adstocks, groups,time_id,options, skip, id) {
  step(
    subclass = "adstock", 
    terms = terms,
    role = role,
    trained = trained,
    adstocks=adstocks,
    retention=retention,
    groups=groups,
    time_id=time_id,
    options = options,
    skip = skip,
    id = id
    
  )
}

#' Prep method for step_adstock
#' @param x step or recipe
#' @param training dataset used for prep
#'
#' @return
#' a recipe step of type adstock
#' @export
#' @export prep.step_adstock
#' @example
#' 
#' @importFrom recipes prep
#' 
prep.step_adstock <- function(x, training, info = NULL, ...) {
  col_names <- recipes_eval_select(x$terms, training, info) 
  
  ## We'll use the names later so make sure they are available
  if (x$options$names == FALSE) {
    rlang::abort("`names` should be set to TRUE")
  }
  
  step_adstock_new(terms=x$terms,
                   role=x$role,
                   trained=TRUE,
                   retention=x$retention,
                   groups=x$groups,
                   time_id=x$time_id,
                   adstocks=col_names,
                   options=x$options,
                   skip=x$skip,
                   id=x$id
  )
}
#' function used to compute adstock transformation
#' 
#' @param x numeric vector or time series of values to be adstocked
#' @param retain between 0 and 1, the amount of previous time points value that effects the current time point
#'
#' @return
#' a numeric vector that shows the adstocked values of x
#' @export
#' 
get_adstock<-function(x,retain){
  x<-as.numeric(stats::filter(x,retain,'recursive'))
  return(x)
}
#' bake method for step_adstock
#' 
#' @param object is recipe 
#' @param new_data data.frame or tibble to be baked
#'
#' @return
#' data that is transformed
#' @export
#' @export bake.step_adstock
#' 
#' @example
#' 
#' @importFrom recipes bake
#' @importFrom rlang abort
#' @importFrom rlang warn
#' 
bake.step_adstock<-function(object,new_data,...){
  
  
  vars<-names(object$adstocks)
  groupings<-object$groups
  this_time<-object$time_id
  
     
  if(length(groupings)==0){
    rlang::warn("No grouping vars in data for step_adstock -- assumes data is one continous time series!!!  \nIf this isn't true, group and sort the data appropriately!")
    new_data<-new_data |>
      arrange(across(all_of(this_time)))
    
    stocks<- new_data |> reframe(across(all_of(vars),
                                        function(x){get_adstock(x,object$retention)}))

    new_data[,vars]<-stocks[,vars]
    } else{
  
     #check if we have ragged time intervals
     new_data|>arrange(across(all_of(c(groupings,this_time)))) |>
       dplyr::group_by(across(all_of(groupings))) |>
       mutate(lag_time=lag(across(all_of(this_time))))->intermediate
     intermediate$delta=as.numeric(unlist(intermediate$lag_time))-
       as.numeric(unlist(intermediate[this_time]))
     intermediate|> summarise(min=min(delta,na.rm=T),max=max(delta,na.rm=T))|>mutate(
       range=max-min) |>filter(range!=0,!is.na(range),!is.infinite(min),!is.infinite(max))->problems
     if(nrow(problems)>0){
       rlang::abort('non-constant time intervals will not work with adstock')}
     
  new_data<-new_data |>  dplyr::group_by(across(all_of(groupings)))|> 
    arrange(across(all_of(c(groupings,this_time))))
  
  
  stocks<- new_data |> reframe(across(all_of(vars), 
                                  function(x){get_adstock(x,object$retention)} ))
  
  new_data[,vars]<-stocks[,vars]
   }
  return(new_data)
}


#' custom print method for adstock steps
#' @param x is recipe or step
#'
#' @return
#' descriptive output to console
#' @export
#' @export print.step_adstock
#' @example
#' 

print.step_adstock <-
  function(x, width = max(20, options()$width - 35), ...) {
    
    print_step(
      # Names before prep (could be selectors)
      untr_obj = x$terms,
      # Names after prep:
      tr_obj = names(x$adstocks),
      # Has it been prepped? 
      trained = x$trained,
      # An estimate of how many characters to print on a line: 
      width = width,
      title=paste("Adstock Transformation with retention",x$retention,"on"),
      case_weights=x$case_weights
    )
    invisible(x)
  }

#' tunable method for step_adstock
#' @param x is a step or recipe
#'
#' @return
#' a tibble interpretable by dials and tune as hyperparameter information
#' @export
#' @export tunable.step_adstock
#' 
#' @importFrom tune tunable
#' 
tunable.step_adstock <- function (x, ...) {
  tibble::tibble(
    name = c("retention"),
    call_info = list(list( fun = "retention")),
    source = "recipe",
    component = "step_adstock",
    component_id = x$id
  )
}

#' establishing retention as a dial using new_quant_param
#'
#'@param range is the allowed range of retention for a specific hyperparameter
#'
#' @return
#' a quantitative tunable hyperparameter
#' @export
#' @importFrom dials new_quant_param
#' 
retention<-function(range=c(0,.8)){dials::new_quant_param(type='double',range=range,inclusive=c(TRUE,TRUE),
                                                   label=c(retention='retention'),finalize = NULL)}


```

```{r examples-adstock}
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(recipes)))
suppressMessages(suppressWarnings(library(tune)))
suppressMessages(suppressWarnings(library(dials)))
suppressMessages(suppressWarnings(library(mostlytidyMMM)))
#create two datasets:
mktdata<-rbind(tibble(prod='brand',store='store1',
                      sales=c(100.,100.,100.,100.,100.),
                      tv=c(10.,100.,0.,0.,100),
                      search=c(0,10,20,50.,50.),
                      week=c(1,2,3,4,5)) ,
               tibble(prod='brand',store='store2',
                      sales=c(10.,10,10,10,10),tv=c(0.,0,0,0,0),
                      search=c(0.,2,2,0,0) ,
                      week=c(1,2,3,4,5)) ) |> 
  group_by(prod,store)

mktdata2<-tibble(prod='brand',store='all',sales=100,tv=1000,search=1000,week=6) |> group_by(prod,store)

#build a recipe with two different adstock steps -- could be done in one step
rec_obj <-  recipe(sales ~ ., data = mktdata) |> step_adstock(tv,retention=.5,groups=c('prod','store')) |>
  step_adstock(search,retention=.5,groups=c('prod','store'),time_id='week') |>
  prep(training = mktdata)

#showing off the custom print function
print(rec_obj)

#see the final transformed output
bake(rec_obj,mktdata)

#note what happens to store = 'all' in mktdata2:
bake(rec_obj,mktdata2)

#check to make sure that per-variable application of the step_adstock and step-saturation do not break the grouping structure even though
#the output tibble from bake is ungrouped (irritatingly)

rec_both_steps<-recipe(sales~.,data=mktdata) |> 
  step_adstock(tv,retention=.1) |> step_saturation(tv,asymptote=1000,saturation_speed=.001) |>
  step_adstock(search,retention=.1,time_id='week')  |> prep()


bake(rec_both_steps,mktdata)

#And adstock steps throw an error if time series isn't regular -- last point is two weeks 
mktdata3<-rbind(tibble(prod='brand',store='store1',
                      sales=c(100.,100.,100.,100.,100.),
                      tv=c(10.,100.,0.,0.,100),
                      search=c(0,10,20,50.,50.),
                      week=c(1,2,3,4,6)) ,
               tibble(prod='brand',store='store2',
                      sales=c(10.,10,10,10,10),tv=c(0.,0,0,0,0),
                      search=c(0.,2,2,0,0) ,
                      week=c(1,2,3,4,6)) ) |> 
  group_by(prod,store)

#not run to allow vignette to knit -- uncomment to see error

# rec1<-recipe(mktdata3) |> step_adstock(search,retention=.1,groups=c('prod','store'),
#                                         time_id='week')|>prep()

#bake(rec1,mktdata3)
```


# Handling Roles and Steps in Bulk

It is not uncommon for a Marketing Mix Model to have dozens of variables.  The functions allow roles and media transformations (ie the adstock and saturation steps) to be applied based on the configuration tables.


```{r functions-for-working-in-recipes}
#functions to loop over controls tibbles and create recipe steps

#' loops over the variable control table to assign roles within a recipe.  These
#' roles control transformations applied and construction of model formulas
#' 
#' @param this_recipe a recipe object
#' @param vars_to_append_roles Defaults to var_controls$varname; a vector of variable names that will have updated roles
#' @param roles_to_be_appended Defaults to var_controls$role; a vector of roles that match the vars_to_append_roles vector
#' 
#' @return a recipe

#' @export
#' 
#' @example
#' 
bulk_update_role<-function(this_recipe,vars_to_append_roles=var_controls$varname,roles_to_be_appended=var_controls$role){
  unique_roles<-unique(roles_to_be_appended)
  for (i in 1:length(unique_roles)){
    this_role<-unique_roles[i]
    this_recipe<-this_recipe |> update_role(vars_to_append_roles[roles_to_be_appended==this_role],
                                             new_role=this_role)
  }
  return(this_recipe)
}

#' loops over the variable control table to assign _second_ rolls within a recipe. These roles control transformations applied and construction of model formulas
#' 
#' @param this_recipe a recipe object
#' @param vars_to_append_roles Defaults to var_controls$varname[!is.na(var_controls$role2)]; a vector of variable names that will have updated roles
#' @param roles_to_be_appended Defaults to var_controls$role2[!is.na(var_controls$role2)]; a vector of roles that match the vars_to_append_roles vector
#' 
#' @return a recipe
#' 
#' @export
#' 
#' @example
#' 
bulk_add_role<-function(this_recipe,vars_to_append_roles=var_controls$varname[!is.na(var_controls$role2)],roles_to_be_appended=var_controls$role2[!is.na(var_controls$role2)]){
  unique_roles<-unique(roles_to_be_appended)
  for (i in 1:length(unique_roles)){
    this_role<-unique_roles[i]
    this_recipe<-this_recipe |> add_role(vars_to_append_roles[roles_to_be_appended==this_role],
                                          new_role=this_role)
  }
  return(this_recipe)
}


#' loops over the variable control table to add media transformations based on either variable specific transformations or tunable ranges
#' @param this_recipe a recipe object
#' @param var_specfic_controls Defaults to var_controls; a tibble containing variable name, role, role2, asymptote, saturation_speed, sign , and retention values (or blanks)
#' @param media_controls Defaults to transform_controls; a tibble containing groups of variables (ie role2 values) that assignes ranges to asymptote, saturation_speed, and retention IF there are no values for the individual variables
#' 
#' @return a recipe
#' 
#' @export
#' 
#' @example
#' 
add_steps_media<-function(this_recipe,var_specific_controls=var_controls,media_controls=transform_controls){
  
  #get time_id -- critical for adstock steps
  this_time_id<-var_specific_controls |> filter(role=='time_id') |>
    select(varname)|>unlist()
  
  #two cases -- specific variable settings in var_specific_controls or not.
  
  #find var specific controls here.  
  
  var_specific_controls <-var_specific_controls |> select(varname,retention,asymptote,saturation_speed) |> 
    mutate(across(c(retention,asymptote,saturation_speed),as.numeric) )|> 
    filter(!is.na(retention) | !is.na(asymptote) |!is.na(saturation_speed)) 
  
  n_specific_controls<-nrow(var_specific_controls)
  
  vars_to_skip<-vector('character') #this will hold vars with specific settings so they are skipped when doing group -based transforms
  
  these_groups<-as.character(groups(this_recipe$template))
  
  if(n_specific_controls>0){
    for(specific in 1:n_specific_controls){
      this_row<-var_specific_controls[specific,c('varname','asymptote','saturation_speed','retention')]
      this_var<-var_specific_controls$varname[specific]
      asymptote_id=paste0(this_var,'_asymptote')
      saturation_speed_id=paste0(this_var,'_saturation_speed')
      retention_id=paste0(this_var,'_retention')
      if(is.na(var_specific_controls$asymptote[specific])){this_asymptote<-tune(asymptote_id)}else{this_asymptote<-var_specific_controls$asymptote[specific]}
      if(is.na(var_specific_controls$saturation_speed[specific])){this_saturation_speed<-tune(saturation_speed_id)}else{this_saturation_speed<-var_specific_controls$saturation_speed[specific]}
      if(is.na(var_specific_controls$retention[specific])){this_retention<-tune(retention_id)}else{this_retention<-var_specific_controls$retention[specific]}
      
      this_recipe<-this_recipe |> step_adstock(!!this_var,retention=this_retention,groups = these_groups,time_id=this_time_id) |>
        step_saturation(!!this_var,asymptote=this_asymptote,saturation_speed=this_saturation_speed)
    }
    #don't need to add steps for any of those variables, so make list to keep out in the settings by role
    vars_to_skip<-var_specific_controls$varname
  }
  
  if(nrow(media_controls)>0){
    for(group_no in 1:nrow(media_controls)){
      this_role<-media_controls$role[group_no]
      asymptote_id<-paste0(this_role,'_asymptote')
      saturation_speed_id<-paste0(this_role,'_saturation_speed')
      retention_id<-paste0(this_role,'_retention')
      
      if(media_controls$retention_high[group_no]==media_controls$retention_low[group_no]){
        this_retention<-media_controls$retention_high[group_no]}else{this_retention<-tune(retention_id)}
      if(media_controls$asymptote_high[group_no]==media_controls$asymptote_low[group_no]){
        this_asymptote<-media_controls$asymptote_high[group_no]}else{this_asymptote<-tune(asymptote_id)}
      if(media_controls$saturation_speed_high[group_no]==media_controls$saturation_speed_low[group_no]){
        this_saturation_speed<-media_controls$saturation_speed_high[group_no]}else{this_saturation_speed<-tune(saturation_speed_id)}
      
      if(length(vars_to_skip)>0){  
       vars_with_role<-summary(this_recipe) |> group_by(variable) |> 
          summarise(roles=paste(role,collapse=',')) |> 
         filter(grepl("predictor",roles,fixed=T),grepl(this_role,roles,fixed=T),
                !(variable %in% !!vars_to_skip) )|> select(variable) |> distinct() |> unlist()
        if(length(vars_with_role)>0){
          this_recipe<-this_recipe |> step_adstock(all_of(vars_with_role),
                                                    retention = this_retention,groups = these_groups,time_id=this_time_id) |> 
            step_saturation(all_of(vars_with_role),
                      asymptote=this_asymptote,
                      saturation_speed=this_saturation_speed)
          }
       }
      else{
        this_recipe<-this_recipe |> step_adstock(has_role(!!this_role),
                                                  retention = this_retention,groups=these_groups,time_id=this_time_id) |> 
          step_saturation(has_role(!!this_role),
                          asymptote=this_asymptote,
                          saturation_speed=this_saturation_speed)
      }
    }
  }
  return(this_recipe)
}

#' renames columns in data per the controls file
#' @param working_df is the dataset that needs to be renamed in a tibble or data.frame
#' @param variable_controls Defaults to var_controls; a tibble containing start_name and varname, with varname being the desired final name of each columns
#' @return
#' a tibble with (potentially) renamed columns
#' 
#' @export
#' 
#' @example
rename_columns_per_controls<-function(working_df,variable_controls=var_controls){
  new_names<-variable_controls$varname[match(names(working_df),variable_controls$start_name)]
  #catch for names not in the controls table
  na_idx<-which(is.na(new_names))
  new_names[na_idx]<-names(working_df)[na_idx]
  #rename
  names(working_df)<-new_names
  return(working_df)
}

#' computes variables suitable for seasonality predictors
#' @param data_to_use defaults to data1; suitable for MMM and with variables matching the vc table
#' @param vc deafults to var_controls; tibble with varname and role indicating which
#' variable is the time_id column
#' @export
#' @example
#' 
add_fourier_vars<-function(data_to_use=data1,vc=var_controls){
  time_id_var <-vc|> filter(role=='time_id') |> select(varname) |> unlist()
  
  time_id_vec<-data_to_use |> ungroup() |> select(all_of(time_id_var)) |> unlist()
  if( !is.Date(time_id_vec) &  ! is.numeric(time_id_vec)){stop("non date vector used as time_id var.  
                                  For variable set as time_id var (ie where role=time_id), please reset it to be a date  ")}
  
  if(is.Date(time_id_vec)){data_to_use$day_int=lubridate::yday(time_id_vec)}else{
    data_to_use$day_int=time_id_vec
  }
  return(data_to_use |> mutate(cos1=cos(2*pi*day_int/356),
                                cos2=cos(4*pi*day_int/356),
                                cos3 =cos(6*pi*day_int/356),
                                cos4 = cos(8*pi*day_int/356),
                                cos5 = cos(10*pi*day_int/356),
                                sin1=sin(2*pi*day_int/356),
                                sin2=sin(4*pi*day_int/356),
                                sin3=sin(6*pi*day_int/356),
                                sin4=sin(8*pi*day_int/356),
                                sin5=sin(10*pi*day_int/356)) |> select(-day_int))
}



#' sorts and groups data per the variable controls table
#' 
#' @param data_to_use defaults to data1;  a tibble or tidytable suitable for MMM and matching the 
#' variables listed in vc (the variable congtrol table)
#' @param vc defaults to var_controls; a tibble with varname, role, role2, describing
#' the roles variables will play in the MMM
#' @export
#' @return
#' a tibble 
#' @importFrom dplyr filter
#' @importFrom dplyr select
#' @importFrom dplyr mutate
#' @importFrom dplyr across
#' @importFrom dplyr all_of
#' @importFrom dplyr arrange
#' @importFrom dplyr group_by
add_groups_and_sort<-function(data_to_use=data1,vc=var_controls){
  #extract groups from vc
  groupings<-vc |> filter(role2=='group') |> select(varname) |> unlist()
  names(groupings)<-NULL
  time_id_var <-vc|> filter(role=='time_id') |> select(varname) |> unlist()
  if(length(groupings)>0){
    
    data_to_use<-data_to_use|> 
      mutate(across(all_of(!!groupings),as.factor))
    
    return(data_to_use |>  dplyr::group_by(across(all_of(groupings))) |> 
             arrange(across(all_of( c(!!groupings,!!time_id_var)))) )
  }
  else{
    return(data_to_use |> arrange(across(all_of(c(!!time_id_var))) )
           ) }
}

#' checks random terms in workflow_control against data; if fewer than 2 levels for the groups, removes the random terms from search
#' 
#' @param data_of_interest  defaults to data1;  a tibble or tidytable suitable for MMM and matching the 
#' variables listed in vc (the variable congtrol table)
#' @param workflow_controls defaults to workflow_controls; a tibble with Value and R_name describing
#' the various model search options
#' @export
#' @return
#' a tibble 
adjust_workflow_for_groups_not_in_data<-function(workflow_controls=workflow_controls,data_of_interest=data1){
  all_random_terms <- workflow_controls |> filter(R_name %in% c('list_rand_ints','list_rand_slopes')) |> select(Value) |> unlist() |> paste(collapse=',')
  
  #a vector of possible grouping_terms
  grouping_terms <- unique(gsub(")","",gsub("|","", stringr::str_extract_all(all_random_terms,"\\|.+?\\)")[[1]],fixed=T),fixed=T))
  #check if at least two values in data for each terms
  grp_to_remove=vector(mode='character')
  for (gg in 1:length(grouping_terms)){
   
    if(length(unique(data_of_interest[grouping_terms[gg]]))<2){  
      grp_to_remove<-c(grp_to_remove,grouping_terms[gg]) }
  }
  ##Need to remove the rand terms that have any of the grp_to_remove values frmo the workflow_controls list_rand_ints or list_rand_slopes
  if (length(grp_to_remove)>0){
    for (gg in 1: length(grp_to_remove)){
      this_grp<-grp_to_remove[gg]
      slope_terms <- strsplit(workflow_controls$Value[workflow_controls$R_name=='list_rand_slopes'],',')[[1]]#because workflow_controls$Value is length 1
      slope_terms2<-paste0(slope_terms[!(stringr::str_detect(slope_terms,this_grp))],collapse=',')
      workflow_controls$Value[workflow_controls$R_name=='list_rand_slopes']<-slope_terms2
      
      int_terms <- strsplit(workflow_controls$Value[workflow_controls$R_name=='list_rand_ints'],',')[[1]]#because workflow_controls$Value is length 1
      int_terms2<-paste0(int_terms[!(stringr::str_detect(int_terms,this_grp))],collapse=',')
      workflow_controls$Value[workflow_controls$R_name=='list_rand_ints']<-int_terms2
    }
  }
  #cancel search of random terms if they are now blank
  if ( workflow_controls$Value[workflow_controls$R_name=='list_rand_ints']=='' &  workflow_controls$Value[workflow_controls$R_name=='list_rand_ints']==''){
    workflow_controls$Value[workflow_controls$R_name=='search_randoms']<-FALSE
  }
  return(workflow_controls)
}


#'check to see if a recipe needs to be finalized before baking
#'
#'@param recipe_to_check a recipe
#'@return booelan true or false
#'@example
#'@details
#'This function wraps prep(recipe_to_check) in tryCatch to produce a TRUE/FALSE on
#'if the recipe still has tune() in some of its parameters.
#'This will be inefficient in the situation where the data in the recipe is large or the
#'prep steps are lengthy
#'@export
#'@importFrom recipes prep
#'
check_if_needs_tune<-function(recipe_to_check){
  tryCatch({recipes::prep(recipe_to_check)
    FALSE},error=function(e) {
      if(grepl("Argument(s) with `tune()`:",e$message,fixed=T))
      return(TRUE) })
}


#' reads the variable controls configuration and dataset to create a recipe, 
#' which may include tuning dials
#' 
#' @param data_to_use defaults to data1, should be a tibble or tidytable that 
#' has variable names and structure useable as template for the MMM data
#' @param vc defaults to var_controls, should be a tibble describing individual
#' variable roles (ie role and role 2 and varname are included inthe tibble).
#' @param mc defaults to transform_controls; a tibble with hyperparameter ranges
#' by role (ie media type)
#' @param wc defaults to workflow_controls; a tibble with R_name and Value columns
#' where R_name holds control names expected by several of this package's functions
#' and Value is the value of those controls
#' @export
#' 
#' @example
#' 
#' @return a recipe with roles, adstock and saturation steps defined by controls table
#' 
#' @importFrom tune finalize_recipe
#' @importFrom recipes recipe
create_recipe<-function(data_to_use=data1 ,vc=var_controls,mc=transform_controls,
                        wc=workflow_controls){
  #start recipe by assigning roles
  #  small data is good, going to need to loop over recipe repeatedly, lots of internal copying
  
  groupings<-as.character(groups(data_to_use))
  
  recipe0<-recipe(head(data_to_use,n=100) ) 
  
  recipe1<-recipe0 |> bulk_update_role(vars_to_append_roles=vc$varname,roles_to_be_appended=vc$role) |> bulk_add_role(vars_to_append_roles=vc$varname[!is.na(vc$role2)],roles_to_be_appended=vc$role2[!is.na(vc$role2)]) 
  
  recipe2<-recipe1 |> add_steps_media(var_specific_controls=vc,media_controls=mc) |>  step_select(-has_role('postprocess'))
  
  recipe3 <-recipe2  |># step_center(week) |>
    update_role(c(sin1,sin2,sin3,sin4,sin5,cos1,cos2,cos3,cos4,cos5),new_role='time') |> 
    add_role(c(sin1,sin2,sin3,sin4,sin5,cos1,cos2,cos3,cos4,cos5),new_role='predictor') |> 
    step_novel(all_of(!!groupings)) |> 
    step_mutate_at(all_of(!!groupings),fn=list(id=as.integer))
  
  if(get_control('tune_this_time',wc)=='FALSE'){
    if(check_if_needs_tune(recipe3)){
    if(get_control('saved_hypers_filename') =='' | 
       is.na(get_control('saved_hypers_filename') ) ){
      stop("workflow controls calling for skipping tuning but the saved_hypers_filename control is empty!
            Fill that control in with the location of saved hyperparameters")
     }
     recipe3<-recipe3|>tune::finalize_recipe(readRDS(get_control('saved_hypers_filename',wc)))
    }
  }
  return(recipe3)
}

```

```{r examples-working-with-recipes}
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(mostlytidyMMM)))
suppressMessages(suppressWarnings(library(recipes)))
suppressMessages(suppressWarnings(library(dials)))
suppressMessages(suppressWarnings(library(tune)))


#get control spreadsheet
control_file<-system.file('example model control.xlsx',package='mostlytidyMMM')
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')

data_set=rename_columns_per_controls(read.csv(system.file("example2.csv", package = "mostlytidyMMM")),variable_controls=var_controls)


recipe0<-recipe(head(data_set,n=1) )

recipe1<-recipe0 |> bulk_update_role() |> bulk_add_role()
add_steps_media(recipe1)

recipe1

```

# Creating rethinking::ulam inputs

The ulam function generates a stan program and then runs sampling from it.  It is very flexible, giving a user access to most of what stan can do.  create_ulam_list() is a function that translates a lmer()-style formula into the requisite inputs for ulam.

```{r function-for-ulam}
#' converts sign constraints in the variable configuration table to rethinking::ulam ready constraints.
#' @param variable_controls Defaults to var_controls; a tibble containing start_name and varname, and sign, which is a column with constraints as '>=0' or '+'. Currently only supports >=0 or <=0, with +,- being used as alias for those.
#' @return
#' a list of boundary statements suitable for rethinking::ulam
#' 
#' @export
#' @importFrom dplyr filter
#' @importFrom dplyr select
#' @importFrom dplyr mutate
#' @importFrom dplyr across
#' @importFrom dplyr all_of
#' @importFrom dplyr arrange
#' @importFrom dplyr rowwise
#' @example
make_bound_statements<-function(variable_controls=var_controls){
 
  bounded_coefs<-variable_controls |> filter(!is.na(sign)) |> rowwise() |> mutate(
    bound_statement =list(ifelse(sign %in% c('>=0','>0','>','+'),'lower=0',
                                 ifelse(sign %in% c('<=0','<0','<','-'),'upper=0',sign))),
    name_for_list=ifelse(tolower(role)=='outcome',varname,paste0('b_',varname))
  )
  list_of_bounds<-bounded_coefs$bound_statement
  names(list_of_bounds)<-bounded_coefs$name_for_list
  return(list_of_bounds)
}

#' Creates sampling statements for user defined priors on individual variables.
#' 
#' @param variable_controls Defaults to var_controls; a tibble containing start_name and varname, prior, and prior_sd, where the column prior is the mean of a normal dist.
#' @return
#' a list of sampling statements suitable for rehtinking::ulam()
#' 
#' @export
#' 
#' @example
make_prior_statements<-function(variable_controls=var_controls){
  prior_frame<-variable_controls |> filter(!is.na(prior)) |> select(varname,sign,prior,prior_sd) |> mutate(prior_sd=ifelse(is.na(prior_sd),prior*5,prior_sd))
  prior_frame<-prior_frame |> rowwise() |> mutate(coef_name=paste0("b_",varname),
                                                    prior_def=list(as.formula(paste0("b_",varname,"~ normal(",prior,",",prior_sd,")"))))
  
  priors_for_ulam<-prior_frame$prior_def
  names(priors_for_ulam)<-prior_frame$coef_name
  return(priors_for_ulam)
  }



#' From a combination of arguments and the user specified priors, creates the final
#' list of expressions that rethinking::ulam uses to build the stan model block.  Currently only allows additive normal model with all media transformations applied before the regression is run.  Currently only allows for random intercepts.
#' 
#' @param prior_controls Defaults to var_controls; a tibble containing start_name and varname, prior, and prior_sd, where the column prior is the mean of a normal dist.
#' @param model_formula Defaults to the object built_formula; a string containing the model formula in the style of lmer.
#' @param main_error_term_prior defaults to 'half_cauch(0,100)'; should be a sampling distribution from stan and becomes the prior on the residual error
#' @param grand_intercept_prior defaults to 'normal(50,50)'; is a string defining a sampling distribution from stan and becomes the prior on the intercept in the regression
#' @param random_int_stdev_prior defaults to 'half_cauchy(0,10)'; is a string defining the sampling distribution of the std deviation of any included random intercepts.  If there are two groups of 'random intercepts' this will apply to both.
#' @param rand_int_prior_mean defaults to 65; the mean of the prior (normal) for random intercepts. If there are two groups of 'random intercepts' this will apply to both.
#' @param unspecified_priors defaults to 'normal(0,10)'; should be a stan sampling distribution that defines the 'uninformative' prior to use when no other prior is speciied.
#' @param random_slope_stdev_prior defaults to half_cauch(0,10); should be a string describing a stan sampling distribution and is used to define the spread in any random
#' slopes in the model
#' @return
#' a list of expressions suitable for use as the formula list in rethinking::ulam
#' 
#' @export
#' 
#' @importFrom stringr::str_count
#' 
#' @example
create_ulam_list<-function(prior_controls=var_controls, model_formula=built_formula,
                           rand_int_prior_mean=65,
                           main_error_term_prior='half_cauchy(0,100)',
                           grand_intercept_prior= 'normal(50,50)',
                           random_int_stdev_prior ='half_cauchy(0,10)',
                           random_slope_stdev_prior='half_cauchy(0,10)',
                           unspecified_priors='normal(0,10)'
){
  
  #translate user defined priors
  user_defined_priors<-make_prior_statements(variable_controls = prior_controls)
  
  #create list of remaining priors -- 'uninformative' priors to be used here which is just normal(0,10)
  all_terms<-attr(terms(as.formula(model_formula)),'term.labels') 
  all_coefs<-paste0("b_",attr(terms(as.formula(model_formula)),'term.labels') )
  
  #random ints, i.e. 1|<var> we need to make the prior on <var>_int[<var>_id] and the deterministic formula will be <var>_int[<var>_id]
  random_ints0<- all_coefs[grep("b_1 | ",all_coefs,fixed=T)]
  random_ints<-gsub("^.*b_1 \\| ","",random_ints0)
  
  if(length(random_ints0)>0){
    priors_for_random_ints<-lapply(paste0(random_ints,'_int[',random_ints,'_id] ~ normal(',rand_int_prior_mean,',int_sigma)'),as.formula)
    names(priors_for_random_ints)<-random_ints
  }else{
    priors_for_random_ints=list()
  }
  #random slopes, i.e. b_var|group_var
  random_slopes0<-all_coefs[grepl("\\|",all_coefs) & !grepl("b_1 \\|",all_coefs)]
   #split into grouping vars and new 'coeff' with coeff name +_interact_<group name>
  random_slope_group_vars<-trimws(sub('.+\\|(.+)', '\\1', random_slopes0))
  random_slope_idvars<- paste0(random_slope_group_vars,"_id")
  
  random_slope_real_vars<-paste0(trimws(sub('(.+)\\|.+', '\\1', random_slopes0)),"_interact_",random_slope_group_vars)
  
  if(length(random_slopes0)>0){
    priors_for_random_slopes<-lapply(
      paste0(random_slope_real_vars,'[',random_slope_idvars,'] ~ normal(',
             0,',slope_sigma)'),as.formula)
    names(priors_for_random_slopes)<-random_slope_real_vars
  }else{
    priors_for_random_slopes=list()
  }
  #get diffuse priors on fixed effects not specified by user
 fixed_coefs<-all_coefs[!(all_coefs %in% random_ints0) & !(all_coefs %in% random_slopes0)]
  fixed_terms<-all_terms[!(all_coefs %in% random_ints0) & !(all_coefs %in% random_slopes0)]
   
  
  fixed_coefs_needing_priors<-fixed_coefs[!(fixed_coefs %in% names(user_defined_priors))]
  
  priors_for_fixed<-lapply(paste0(fixed_coefs_needing_priors,"~",unspecified_priors),as.formula)
  names(priors_for_fixed)<-fixed_coefs_needing_priors
  
  main_model_formula<-as.formula(paste0(as.character(as.formula(model_formula))[2],'~ normal(big_model,big_sigma)'))
  #prior on big_sigma
  prior_on_big_sigma<-as.formula(paste('big_sigma ~ ',main_error_term_prior))
  #prior on grand intercept
  prior_on_a0<-as.formula(paste('a0 ~ ',grand_intercept_prior))
  
  #prior_on_store_ints_spread
  prior_on_store_int_sigma<-as.formula(paste('int_sigma ~',random_int_stdev_prior))
  #prior on random slope spread
  prior_on_slope_sigma<-as.formula(paste('slope_sigma ~',random_slope_stdev_prior))
  #more than around 15 terms to sum for the expected value in the model, need to split it rethinking::ulam builds an incorrect stan file.
  
  
  number_of_terms<-length(all_terms)
  
  rand_ints_formula_for_ulam<-  ifelse(length(random_ints0)>0,
                                       paste(paste0(random_ints,"_int[",random_ints,"_id]"),collapse='+'),
                                       '')
  rand_slopes_formula_for_ulam<-ifelse(length(random_slopes0)>0,
                                       paste(paste0(random_slope_real_vars,'[',random_slope_idvars,']'),collapse=' + '),
                                       '')
  
  number_of_fixed<-number_of_terms-
    stringr::str_count(rand_slopes_formula_for_ulam,"\\[") - 
    stringr::str_count(rand_ints_formula_for_ulam,"\\[") 
  
  big_model_list<-vector('list')
  included_terms=0
  start_term=1
  last_end_term=15
  iter_of_big_model=1
  while(start_term<=number_of_fixed){
    
    this_end_term=min(last_end_term,number_of_fixed)
    if(iter_of_big_model==1){
      big_model_list[[iter_of_big_model]]<- paste(paste0('big_model_',iter_of_big_model,' <-'),paste(fixed_coefs[start_term:this_end_term],
                                                                                                     fixed_terms[start_term:this_end_term],sep='*',collapse='+'))
    }
    else{
      big_model_list[[iter_of_big_model]]<-paste(paste0('big_model_',iter_of_big_model,' <- big_model_',iter_of_big_model-1,' + '),paste(fixed_coefs[start_term:this_end_term],
                                                                                                                                         fixed_terms[start_term:this_end_term],sep='*',collapse='+'))
    }
    start_term=this_end_term+1
    last_end_term=this_end_term+15
    iter_of_big_model=iter_of_big_model+1
  }
  if(rand_ints_formula_for_ulam!='' & rand_slopes_formula_for_ulam != '')
  {big_model_list[[iter_of_big_model]]<-paste(paste0('big_model <- big_model_',
                                                     iter_of_big_model-1),'a0',
                                              rand_ints_formula_for_ulam,
                                              rand_slopes_formula_for_ulam,sep='+')}
  else if(rand_ints_formula_for_ulam!=''){
    big_model_list[[iter_of_big_model]]<-paste(paste0('big_model <- big_model_',
                                                      iter_of_big_model-1),'a0',
                                               rand_ints_formula_for_ulam,sep='+')
  }else if(rand_slopes_formula_for_ulam!=''){
    big_model_list[[iter_of_big_model]]<-paste(paste0('big_model <- big_model_',
                                                      iter_of_big_model-1),'a0',
                                               rand_slopes_formula_for_ulam,sep='+')
  }
    else
    {big_model_list[[iter_of_big_model]]<-paste(paste0('big_model <- big_model_',
                                                     iter_of_big_model-1),'a0',sep='+')}
  
  #to have expressions in the final list, need to prase the strings in big_modl
  big_model_list_parsed<-sapply(big_model_list,function(x) parse(text=x))
  
  formula_list<-c(main_model_formula,rev(big_model_list_parsed),priors_for_random_slopes,priors_for_random_ints,priors_for_fixed,user_defined_priors,prior_on_a0,prior_on_big_sigma,prior_on_store_int_sigma,prior_on_slope_sigma)
  
  class(formula_list)<-'list'
  return(formula_list)
}


#'a prediction function for ulam objects, as comes from rethinking::ulam
#'
#'@param ulamobj the out of rethinking::ulam
#'@param new_data the dataset on which to draw samples and make predictions
#'@param n the number of samples to draw
#'@param reduce defuaults to TRUE; a boolean that indicates if the samples should be returned or a mean and high credibility interval
#'@param conf defaults to .95, indicates the desired converage of credibility interval when reduce=T
#'
#'@return
#'if reduce=T a tibble with pred, pred_lower_conf, pred_upper_conf columns
#'else if reduce =F the output of link (a matrix with sampled predictions)
#'
#'@export
#'@export predict.ulam
#'@importFrom rethinking link
#'@importFrom stats predict
predict.ulam<-function(ulamobj,new_data,n=1000,reduce=T,conf=.95){
  link_output<-link(ulamobj,new_data=new_data,n=n)$big_model
  if(reduce){
    preds<-colMeans(link_output)
    low_conf<-apply(link_output,2,quantile,1-conf)
    high_conf<-apply(link_output,2,quantile,conf)
    pred_output<-cbind(preds,low_conf,high_conf)
    colnames(pred_output)<-c('pred',paste0('pred_lower_',conf),paste0('pred_upper_',conf))
  }
  return(pred_output)
}

#'pulls predictor variable names from a recipe except those that have role2 %in% c('group','time')
#'
#'@param base_recipe defaults to recipe3; is the recipe to pull predictor names from
#'
#'@export
#'
#' @importFrom dplyr filter
#' @importFrom dplyr select
#' @importFrom dplyr mutate
#' @importFrom dplyr across
#' @importFrom dplyr all_of
#' @importFrom dplyr arrange
#'
#'@return a vector of variablnames that have role == 'predictor'
#'
#'@example
get_predictors_vector<-function(base_recipe=recipe3){
  groups_and_time<-unlist(summary(base_recipe) |> filter(role %in% c('group','time')) |> select(variable))
  
  predictors<-unlist(summary(base_recipe) |> 
                       filter(role=='predictor') |> select(variable))
  names(predictors)<-NULL
  return(predictors[!(predictors %in% groups_and_time)])
}

#'pulls vector of identification variables from recipe using role== group or role==time_id
#'@param base_recipe defaults to recipe3; a recipe produced in the mostlytidyMMM format
#'@return
#' a character vector
#' @example 
#' @export
#' @importFrom dplyr filter
#' @importFrom dplyr select
#' @importFrom dplyr mutate
#' @importFrom dplyr across
#' @importFrom dplyr all_of
#' @importFrom dplyr arrange
get_id_vector<-function(base_recipe=recipe3){
  groups_and_time<-unlist(summary(base_recipe) |> filter(role %in% c('group','time_id')) |> select(variable))
  names(groups_and_time)<-NULL
    return(groups_and_time)
}


#'pulls outcome variable from recipe using role== group or role==time_id
#'@param base_recipe defaults to recipe3; a recipe produced in the mostlytidyMMM format
#'@return
#' a character vector
#' @example 
#' @export
#' @importFrom dplyr filter
#' @importFrom dplyr select
#' @importFrom dplyr mutate
#' @importFrom dplyr across
#' @importFrom dplyr all_of
#' @importFrom dplyr arrange
get_outcome_vector<-function(base_recipe=recipe3){
  groups_and_time<-unlist(summary(base_recipe) |> filter(role %in% c('outcome')) |> select(variable))
  names(groups_and_time)<-NULL
  return(groups_and_time)
}
#' Pulls values from the control tibble by name; convenience function for working with the control tibble.
#' 
#' @param this_control is the string containing the control of interest
#' @param control defaults to workflow_controls; is a tibble containing columns R_name and Value, 
#' @return
#' The value of control$Value when control$R_name==this_control; typically will be a character, frequently requires coercion to other types.
#' @export
#' @importFrom dplyr filter
get_control<-function(this_control,control=workflow_controls){
  if(length(this_control)==0){stop("get_control requires this_control to be non-null")}
  control |> filter(R_name==!!this_control) |> select(Value) |> unlist()
}


#' Creates a string that represents a model formula from a recipe and the workflow controls data
#' 
#' @param base_recipe defaults to recipe3; is the recipe containing variables to build a formula for
#' @param control defaults to workflow_controls, should be a tibble with columns R_name and Value, which must have rows with R_name =='Y','list_rand_ints' and 'fft_terms'
#' @param ignore_rands defaults to FALSE, but if set to TRUE the created formula 
#' will not contain random intercepts or slopes other than fourier transform 
#' related slopes (if those are specified in the control frame)
#' @return
#' a string that reads like an lmer formula
#' @export
#' 
#' @example
create_formula<-function(base_recipe=recipe3,control=workflow_controls,ignore_rands=FALSE){
  #we will remove grouping vars (used for random effects) and time series stuff (for now)
  
  final_predictors<-get_predictors_vector(base_recipe=base_recipe)
  outcome<-get_control(this_control='Y',control=control)  
  fft_interaction_term<-get_control("interaction_fft",control=control)
  
  fft_terms<-get_control("fft_terms",control=control) |> as.numeric()
  fft_terms<-ifelse(is.na(fft_terms),0,fft_terms)
  
  fft_formula<-""
  
  if(fft_terms>0){
    for(i in 1:fft_terms){
      if (i==1){
        if(fft_interaction_term!="" & !is.na(fft_interaction_term)){ fft_formula=
            paste0("(sin1|",fft_interaction_term,") + (cos1|",fft_interaction_term,")")}
        else{  fft_formula= 'sin1 + cos1'}
      }
      else{
        if(fft_interaction_term!="" & !is.na(fft_interaction_term)){
          new_term<-paste(paste0('(',c('cos','sin'),i,'|',fft_interaction_term,')'),sep='',collapse='+')
          fft_formula=paste(fft_formula,new_term,sep='+')
        }
        else{
        fft_formula=paste(fft_formula,paste(c('cos','sin'),i,sep='',collapse='+'),sep='+')
        }
      }}}
  
  list_rand_ints<-gsub(" ","",get_control("list_rand_ints",control=control),fixed=T) |> strsplit(',',fixed=T) |> unlist()
  list_rand_ints<-list_rand_ints[!is.na(list_rand_ints)]
  
  rand_int_formula<-""
  if(length(list_rand_ints)>0){
    rand_int_formula<-paste(list_rand_ints,collapse =' + ')
  }
  
  list_rand_slopes<-gsub(" ","",get_control("list_rand_slopes",control=control),fixed=T) |> strsplit(',',fixed=T) |> unlist()
  list_rand_slopes<-list_rand_slopes[!is.na(list_rand_slopes)]
  
  rand_slope_formula<-""
  if(length(list_rand_slopes)>0){
    rand_slope_formula<-paste(list_rand_slopes,collapse =' + ')
  }
  
  if(!ignore_rands){
   built_formula<-paste(paste0(outcome,' ~ ',
                              paste(final_predictors,collapse=' + ')),
                       fft_formula,rand_int_formula,rand_slope_formula,sep=' + ')
  }
  if(ignore_rands){
    built_formula<-paste(paste0(outcome,' ~ ',
                                paste(final_predictors,collapse=' + ')),
                         fft_formula,sep=' + ')
  }
  #remove trailing +, left by the paste in built_formula if one of fft_formula, 
  #rand_int_formula, or rand_slope_formula are empty strings
  total_times<-(length(list_rand_slopes)==0)+(length(list_rand_ints)==0)+(fft_terms==0)
  
  if (total_times>0){
    for(i in 1:total_times){
      built_formula<-gsub("(\\+\\s* $)","",built_formula)
    }
    built_formula<-trimws(built_formula)
  
  }
  built_formula <- gsub("\\+  \\+", "\\+", built_formula)  
  return(built_formula)
}


#' produce a list of  formula for testing combinations of random effects
#' 
#' @param vc defaults to best_seas vc; a tibble with a single seasonality specification typically determined
#' by the seasonality_search process
#' @param seasonality_formula defaults to best_seas_formula; a string containing the formula for
#' all fixed effects in the model plus the seasonaity terms (which may include random slopes).
#' Typically will be produced by tuning a workflowset to identify the best seasonality specification.
#' @export
#' @return a list of strings suitable to be coerced to formula for lmer
#' and the specifications for random effects.  In the case where the control
#' search_randoms = 'FALSE' , the string in seasonality_formula is returned in a list of 1
#' 
make_list_of_rands_formula<-function(seasonality_formula=best_seas_formula,
                                     vc=best_seas_vc){
  
  if(get_control("search_randoms",vc)=="FALSE"){
    #in this case we expect that the seasonality_formula will already contain
    # any random effects specified (unless some process has problematically tampered
    #with the search_randoms control)
    
    random_terms=gsub(',','+',paste(get_control("list_rand_ints",vc),
                                    get_control('list_rand_slopes',vc),sep=','),fixed=T)
    
    list_of_formulae_rands<-list(seasonality_formula)
    cat("Nota Bene: make_list_of_rands_formula has been called 
        when search_randoms is FALSE\n Output is a single formula string matchting seasonality_formula.")
  }
  
  else{
    list_of_formulae_rands<-vector('list')
    #get all possible terms
    terms_to_add<-paste(get_control("list_rand_ints",vc),
                        get_control('list_rand_slopes',vc),sep=',') %>% strsplit(split=',') %>% unlist()
    #create all_combinations using collapsed combn output
    pastey<-function(x){paste(unlist(x),collapse=" + ")}
    all_combinations<-unlist(lapply(1:length(terms_to_add),function(x) combn(terms_to_add,x,FUN=pastey,simplify = F)))
    
    for(i in 1:length(all_combinations)){
      list_of_formulae_rands[[i]]<-paste0(seasonality_formula,'+',all_combinations[[i]])
    }
    list_of_formulae_rands<-append(list_of_formulae_rands,seasonality_formula)
  }
  return(list_of_formulae_rands)
}

#' creates a tidymodels workflow from a a recipe and a model
#' @param this_formula defaults to built_formula; a string suitable to be coerced to a formula for lmer
#' @param recipe_to_use defaults to recipe3; a recipe with roles for predictor, outcome, time_id, and groups
#' @export
#' @example 
#' @importFrom workflows workflow
assemble_workflow<-function(this_formula=built_formula,recipe_to_use=recipe3){
  if(grepl("\\|",this_formula)){tune_spec<-linear_reg(engine='lmer')} else{
    tune_spec<-linear_reg(engine='lm')}
  #  hardhat::extract_parameter_set_dials(reg_wf)|> finalize(data1) 
  #need to build a formula with random effects specs for stan_glmer
  
  return(workflow() |>  add_recipe(recipe_to_use) |> 
           add_model(tune_spec,formula=as.formula(this_formula)))
}

#' create a list of model formula (per the rules for lmer()) with different
#' seasonality specifications as listed in the configuration tibble.
#' 
#' @param vc defaults to workflow_controls; a tibble with R_name and Value columns
#' where R_name contains the control name and Value is it's value
#' @param recipe_to_use defaults to recipe3; a recipes::recipe with pre-modeling transformations and
#' variable roles; the outcome, predictor, group, and time_id roles are used in determining
#' the formula.
#' @return a list with names formulae and configs, formulae is a list of strings suitable
#'  to be coerced to a formula and passed to lmer.  configs is a list of tibbles with information like vc,
#'   but reset to match a single seasonality specification.  If search_seasonality is set to FALSE in
#'   the config table, the two lists have a single formula containing the specified seasonality
#'   and no random terms.  A note is printed to the console in this case.
#' @example 
#' @export
#' @importFrom 
make_list_of_fft_formulae<-function(vc=workflow_controls,recipe_to_use=recipe3){
  #use search_randoms control value to determine if formula created here will include
  #random effects not in seaonslity. If search_randoms if TRUE, we do want to ignore
  #random effects specification (for later searching)
  are_we_ignoring_rands = get_control('search_randoms')=="TRUE"
  if(get_control("search_seasonality")=="FALSE"){
    list_of_configs=list(vc)
    formulae=list(create_formula(recipe_to_use,vc,ignore_rands=are_we_ignoring_rands))
    cat("Nota Bene: make_list_off_formulae is called but search_seaonality is FALSE")
    #return(formulae)
  }else{
    fft_interact_options<-get_control("interaction_fft") |> strsplit(split=',',fixed=T) |> unlist()
    if(!("" %in% fft_interact_options)){fft_interact_options=c("",fft_interact_options)}
    
    fft_count_options<-get_control("fft_terms") |> unlist() |> as.numeric()
    #vc<-workflow_controls
    list_of_configs<-vector('list')
    idx=0
    for (this_fterms in 0:fft_count_options){
      for (iterm in 1:length(fft_interact_options)){
        idx=idx+1
        this_iterm=fft_interact_options[iterm]
        
        this_vc=vc |> mutate(Value=ifelse(R_name=='interaction_fft',this_iterm,Value),
                             Value=ifelse(R_name=='fft_terms',this_fterms,Value),
                             Value=ifelse(R_name=='search_seasonality',"FALSE",Value))
        list_of_configs[[idx]]=this_vc
      }
    }
    formulae<-lapply(list_of_configs,function(x) create_formula(recipe_to_use,x,ignore_rands = are_we_ignoring_rands))
    formulae=formulae[-1]
    list_of_configs=list_of_configs[-1]
  }
  return(list(formulae=formulae,configs=list_of_configs))
}


#' get decomps agnostic to multiplicative or additive model specification
#'
#'@param data_to_use defaults to data 3; a transformed dataset (likely produced via bake())
#'@param recipe_to_use defaults to recipe3; a recipe which has the roles assigned for the MMM
#'@param model_obj defaults to rethinking_results; an ulam fit object
#'@param predictors defaulst to get_predictors(recipe3)
#'@param sample_size seems to be useless input to link() function
#'@export
#'@importFrom rethinking link
#'@example
#'@return
#' a tibble with one row per row in data_to_use, incluiding id columns as pulled from
#' the recipe_to_use (ie where role == group or role==time_id), and the one column
#' per predictor variable that shows the amount of the outcome variable due to that predictor.
#' 
#' Additionally, the columns decomp_base, decomp0_tot, pred, and decomp_ratio are appended.
#' 
#' decomp_base is the amount of the outcome variable due to intercept(s) and other no response
#' function needed variables.  decomp0_tot is the sum of the predictor var decomp columns prior
#' to rescaling.  decomp_ratio is the ratio of pred (the full predicted outcome) to decomp0_tot.
#' This has been applied as scaling factor -- it will generally be different from 1 for multiplicative models
#' and be 1 for additive models.
#' @details
#' The decomp algorithm works by first setting all predictors that are not _id variables (as found
#' by get_predictors_vector() to zero and calling link() to determine the prediction for this scenario.
#' 
#' That prediction becomes the initial decomp_base.
#' 
#' Then each predictor is set back to historical values one at a time, and the initial decomp for that
#' variable is set to the difference in prediction with that predictor 'on' and the decomp_base.
#' 
#' Finally, the sum of all the initial decomps is taken, and the ratio between the pred and the sum of decomps
#' is used to scale all decomps (so the sum is equal to prediction).
#' 
#' For additive models, this is equivalent to decomp = model_coef * modeled_independent_variable; for
#' multiplicative models this is equivalent to thinking of decomps as lift over base, with an adjustment to make it
#' additive.
get_decomps_irregardless<-function(data_to_use=data3,recipe_to_use=recipe_finalized,model_obj=rethinking_results,
                             predictors=get_predictors_vector(recipe_to_use),sample_size=1000){
  
  
  #predictors will need to be trimmed of *_id variables (at least until we know how to predict with missinf id values)
  predictors<-predictors[!grepl("_id",predictors,fixed=T)]
  
  variations<-vector('list',length=length(predictors))
  predictors_to_zero<-data_to_use |>
    mutate(across(all_of(!!predictors),function(x) 0 ))
  for(i in 1:length(variations)){
    variations[[i]]<-predictors_to_zero
    variations[[i]][predictors[i]]<-data_to_use[predictors[i]]
  }
  names(variations)<-predictors
  
  decomp_names<-c(names(variations),"decomp_base")
  preds_variations<-lapply(variations,function(x) {colMeans(link(model_obj,x,n=sample_size)$big_model)})
  
  preds_all<-colMeans(link(model_obj,data_to_use,n=sample_size)$big_model)
  
  #intercept_only
 
  decomp_base<-colMeans(link(model_obj,
                                predictors_to_zero,
                                n=sample_size
                                )$big_model)
  
  #this initial delta will be not additive if model is mutiplicative
  list_decomps_0<-lapply(preds_variations,function(x) x-decomp_base)
  decomps_0<-as_tibble(list_decomps_0) 
  #sum decomps_0 . . .
  # decomps_0$int_only<-int_off_preds-preds_main
  
  decomps_0$decomp_base=decomp_base
  decomps_0$decomp0_tot<-rowSums(decomps_0,na.rm=T)
  decomps_0$pred<-preds_all
  decomps_0$decomp_ratio <-  decomps_0$pred/decomps_0$decomp0_tot
  decomps_1<- decomps_0 |>mutate(across(all_of(!!decomp_names),function(x) x*decomp_ratio))
  #to make sure output is clear, need to rename these to decomps_
  #append the columns in data_to_use that are not in decomps_1

  decomps_1<-cbind(data_to_use %>% select(all_of(get_id_vector(recipe_to_use))),
                   data_to_use %>% select(all_of(get_outcome_vector(recipe_to_use))),
        decomps_1)
  return(decomps_1)
}


update_range_from_control<-function(parameter_set,controls){
  for(i in 1:length(parameter_set$id)){
    parameter_set$object[i][[1]]<-do.call(
      unlist(controls[controls$dial_id==parameter_set$id[i],'dial_func']),
      list(range=unlist(controls[controls$dial_id==parameter_set$id[i],'new_range']))
    )
  }
  return(parameter_set)
}

#'Sets hyperparameter ranges for tune grid
#'@param workflow model workflow ore recipe, defaults to recipe3
#'@param control_ranges defaults to transform_controls; a df with ranges by role2 of variable
#'@export
#'
create_dials_from_wf_and_controls<-function(workflow=recipe3,control_ranges=transform_controls){
  tune_these_parms<-extract_parameter_set_dials(workflow) #will have default ranges
  if(nrow(tune_these_parms)==0){return(tune_these_parms)}
  #get ranges from transform_controls
  gamma_ranges<-control_ranges %>%rowwise() %>%  mutate(dial_id=paste0(role,"_saturation_speed")) %>% 
    mutate(new_range=map2(list(saturation_speed_low),list(saturation_speed_high),~c(.x,.y)),
           dial_func='saturation_speed') %>% select(dial_id,new_range,dial_func)
  
  alpha_ranges<-control_ranges %>%rowwise() %>%  mutate(dial_id=paste0(role,"_asymptote")) %>% 
    mutate(new_range=map2(list(asymptote_low),list(asymptote_high),~c(.x,.y)),dial_func='asymptote') %>% select(dial_id,new_range,dial_func)
  
  retention_ranges<-control_ranges %>%rowwise() %>%  mutate(dial_id=paste0(role,"_retention")) %>% 
    mutate(new_range=map2(list(retention_low),list(retention_high),~c(.x,.y)),dial_func='retention') %>% select(dial_id,new_range,dial_func)
  
  
  tune_these_parms<-update_range_from_control(tune_these_parms,rbind(gamma_ranges,alpha_ranges,retention_ranges))
  return(tune_these_parms)
}
```



```{r examples-rehtinking-ulam-related-functions}
suppressMessages(suppressWarnings(library(recipes)))
suppressMessages(suppressWarnings(library(tune)))
suppressMessages(suppressWarnings(library(dials)))
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(mostlytidyMMM)))

#get the control file:
control_file<-system.file('example model control.xlsx',package='mostlytidyMMM')
#get each relevant table of the control file:
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") |> select(-desc)

#pull data, add fourier transform columns

data1<-read.csv(system.file('example2.csv',package='mostlytidyMMM'))|>rename_columns_per_controls(variable_controls=var_controls)|>
  rename_columns_per_controls()|> mutate(week=as.Date(week,"%m/%d/%Y"))|>
  add_fourier_vars(vc=var_controls) |>  add_groups_and_sort(vc=var_controls) 

#append several more columns
lotsa_vars<-paste0('x',1:60)
new_x<-replicate(60,runif(nrow(data1))) 
names(new_x)<-lotsa_vars
new_x<-as_tibble(new_x)

#bring the data together:

data_for_lotsa_vars<-cbind(data1 ,new_x)

#make recipes
recipea<-recipe(head(data_for_lotsa_vars,n=1) )
recipeb<-recipea |> bulk_update_role() |> bulk_add_role()
recipeb<-recipeb |> add_steps_media() |>  step_select(-has_role('postprocess'))
recipec <-recipeb |> step_mutate(week=as.numeric(week)-19247.65) |> 
  update_role(c(sin1,sin2,sin3,cos1,cos2,cos3),new_role='time') |>
  add_role(c(sin1,sin2,sin3,cos1,cos2,cos3),new_role='predictor')
recipec<-recipec |> update_role(starts_with('V'),new_role='predictor')

# build a model formula in the lmer style from the recipe:
(formula_with_lotsa_vars<-create_formula(recipec))

#build a list suitable for input to rethinking::ulam from the formula and recipe:
(create_ulam_list(model_formula=formula_with_lotsa_vars))



```



```{r development-inflate, eval=FALSE}
# Keep eval=FALSE to avoid infinite loop in case you hit the knit button
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
```

