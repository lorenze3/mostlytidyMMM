suppressMessages(suppressWarnings(library(rethinking)))
control_file<-system.file('no_tuning_example.xlsx',package='mostlytidyMMM')
#get each relevant table of the control file:
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") |> select(-desc)
data1<-read.csv(system.file('example2.csv',package='mostlytidyMMM'))|>rename_columns_per_controls(variable_controls=var_controls)|>
rename_columns_per_controls()|> mutate(week=as.Date(week,"%m/%d/%Y"))|>
add_fourier_vars(vc=var_controls) |>  add_groups_and_sort(vc=var_controls)
(no_tuning_recipe<-create_recipe(data1,vc=var_controls,mc=transform_controls,wc=workflow_controls))
(formula_in_a_string<-create_formula(base_recipe=no_tuning_recipe,control=workflow_controls))
(expressions_for_ulam<-create_ulam_list(prior_controls=var_controls,model_formula=formula_in_a_string,
grand_intercept_prior='normal(45,25)') )
(bounds_for_ulam<-make_bound_statements(variable_controls=var_controls))
model_data<-no_tuning_recipe %>% prep() %>% bake(data1)
fitted_model_obj<-ulam(expressions_for_ulam,
model_data,
constraints=bounds_for_ulam,
chains=2,
iter=100,
cores=2,
file='no_tuning_mod',#have a care to remove this if you want to resample!
declare_all_data=F,
messages=F
)
model_data$pred<-predict(fitted_model_obj,model_data)[,1]
suppressMessages(suppressWarnings(library(mostlytidyMMM)))
suppressMessages(suppressWarnings( library(tidyverse)))
suppressMessages(suppressWarnings(library(tidymodels)))
suppressMessages(suppressWarnings(library(rethinking)))
control_file<-system.file('no_tuning_example.xlsx',package='mostlytidyMMM')
#get each relevant table of the control file:
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") |> select(-desc)
data1<-read.csv(system.file('example2.csv',package='mostlytidyMMM'))|>rename_columns_per_controls(variable_controls=var_controls)|>
rename_columns_per_controls()|> mutate(week=as.Date(week,"%m/%d/%Y"))|>
add_fourier_vars(vc=var_controls) |>  add_groups_and_sort(vc=var_controls)
(no_tuning_recipe<-create_recipe(data1,vc=var_controls,mc=transform_controls,wc=workflow_controls))
(formula_in_a_string<-create_formula(base_recipe=no_tuning_recipe,control=workflow_controls))
(expressions_for_ulam<-create_ulam_list(prior_controls=var_controls,model_formula=formula_in_a_string,
grand_intercept_prior='normal(45,25)') )
(bounds_for_ulam<-make_bound_statements(variable_controls=var_controls))
model_data<-no_tuning_recipe %>% prep() %>% bake(data1)
data1
View(model_data)
no_tuning_recipe
View(model_data)
plot(data1$TV1,model_data$TV1)
model_data<-no_tuning_recipe %>% prep() %>% bake(data1)
fitted_model_obj<-ulam(expressions_for_ulam,
model_data,
constraints=bounds_for_ulam,
chains=2,
iter=100,
cores=2,
file='no_tuning_mod',#have a care to remove this if you want to resample!
declare_all_data=F,
messages=F
)
View(data1)
rec<-recipe(data1) %>% step_adstock(TV1,retention=.5,groups=c('store','product'),time_id='week')
rec %>% prep() %>% bake(data1)
rec<-recipe(data1) %>% step_adstock(TV1,retention=.5,groups=c('store','product'),time_id='week') %>%
step_saturation(TV1,asymptote=250,saturation_speed=.0001)
rec %>% prep() %>% bake(data1)
no_tuning_recipe
no_tuning_recipe() %>% prep() %>% bake(data1)
no_tuning_recipe %>% prep() %>% bake(data1)
dplyr::last_dplyr_warnings()
dplyr::last_dplyr_warnings(n = 10)
create_recipe(data1)
no_tuning_recipe$steps
model_data<-no_tuning_recipe %>% prep(data1) %>% bake(data1)
model_data<-no_tuning_recipe %>% prep(data1) %>% bake(data1)
fitted_model_obj<-ulam(expressions_for_ulam,
model_data,
constraints=bounds_for_ulam,
chains=2,
iter=100,
cores=2,
file='no_tuning_mod',#have a care to remove this if you want to resample!
declare_all_data=F,
messages=F
)
model_data$pred<-predict(fitted_model_obj,model_data)[,1]
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/README-",
out.width = "100%"
)
suppressMessages(suppressWarnings(library(mostlytidyMMM)))
suppressMessages(suppressWarnings( library(tidyverse)))
suppressMessages(suppressWarnings(library(tidymodels)))
suppressMessages(suppressWarnings(library(rethinking)))
control_file<-system.file('no_tuning_example.xlsx',package='mostlytidyMMM')
#get each relevant table of the control file:
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") |> select(-desc)
data1<-read.csv(system.file('example2.csv',package='mostlytidyMMM'))|>rename_columns_per_controls(variable_controls=var_controls)|>
rename_columns_per_controls()|> mutate(week=as.Date(week,"%m/%d/%Y"))|>
add_fourier_vars(vc=var_controls) |>  add_groups_and_sort(vc=var_controls)
(no_tuning_recipe<-create_recipe(data1,vc=var_controls,mc=transform_controls,wc=workflow_controls))
(formula_in_a_string<-create_formula(base_recipe=no_tuning_recipe,control=workflow_controls))
(expressions_for_ulam<-create_ulam_list(prior_controls=var_controls,model_formula=formula_in_a_string,
grand_intercept_prior='normal(45,25)') )
(bounds_for_ulam<-make_bound_statements(variable_controls=var_controls))
model_data<-no_tuning_recipe %>% prep(data1) %>% bake(data1)
fitted_model_obj<-ulam(expressions_for_ulam,
model_data,
constraints=bounds_for_ulam,
chains=2,
iter=100,
cores=2,
file='no_tuning_mod',#have a care to remove this if you want to resample!
declare_all_data=F,
messages=F
)
model_data$pred<-predict(fitted_model_obj,model_data)[,1]
model_data<-no_tuning_recipe %>% prep(data1) %>% bake(data1)
fitted_model_obj<-ulam(expressions_for_ulam,
model_data,
constraints=bounds_for_ulam,
chains=2,
iter=100,
cores=2,
file='no_tuning_mod',#have a care to remove this if you want to resample!
declare_all_data=F,
messages=F
)
model_data<-no_tuning_recipe %>% prep(data1) %>% bake(data1)
fitted_model_obj<-ulam(expressions_for_ulam,
model_data,
constraints=bounds_for_ulam,
chains=2,
iter=100,
cores=2,
#file='no_tuning_mod',#have a care to remove this if you want to resample!
declare_all_data=F,
messages=F
)
model_data$pred<-predict(fitted_model_obj,model_data)[,1]
this_rsq<-rsq(model_data|>ungroup(),truth=sales,estimate=pred)['.estimate'] %>% unlist()
this_mape<-mape(model_data|>ungroup(),truth=sales,estimate=pred)['.estimate'] %>% unlist()
ggplot(model_data ,aes(x=sales,y=pred,color=store_id))+
geom_point()+ geom_abline(slope=1,intercept=0)+ggthemes::theme_tufte()+
ggtitle("Predicted vs Actual",subtitle=paste0('Rsq is ',round(this_rsq,2)))
data1<-read.csv(system.file('example2.csv',package='mostlytidyMMM'))|>rename_columns_per_controls(variable_controls=var_controls)|>
rename_columns_per_controls()|> mutate(week=as.Date(week,"%m/%d/%Y"))|>
add_fourier_vars(vc=var_controls) |>  add_groups_and_sort(vc=var_controls)
(no_tuning_recipe<-create_recipe(data1,vc=var_controls,mc=transform_controls,wc=workflow_controls))
(formula_in_a_string<-create_formula(base_recipe=no_tuning_recipe,control=workflow_controls))
no_tuning_recipe<-create_recipe(data1,vc=var_controls,mc=transform_controls,wc=workflow_controls)
(formula_in_a_string<-create_formula(base_recipe=no_tuning_recipe,control=workflow_controls))
create_recipe(data1,vc=var_controls,mc=transform_controls,wc=workflow_controls)
dplyr::last_dplyr_warnings()
create_recipe
#' where R_name holds control names expected by several of this package's functions
#' and Value is the value of those controls
#' @export
#'
#' @example
#'
#' @return a recipe with roles, adstock and saturation steps defined by controls table
#'
#' @importFrom tune finalize_recipe
#' @importFrom recipes recipe
create_recipe<-function(data_to_use=data1 ,vc=var_controls,mc=transform_controls,
wc=workflow_controls){
#start recipe by assigning roles
#  small data is good, going to need to loop over recipe repeatedly, lots of internal copying
groupings<-as.character(groups(data_to_use))
recipe0<-recipe(head(data_to_use,n=100) )
recipe1<-recipe0 |> bulk_update_role(vars_to_append_roles=vc$varname,roles_to_be_appended=vc$role) |> bulk_add_role(vars_to_append_roles=vc$varname[!is.na(vc$role2)],roles_to_be_appended=vc$role2[!is.na(vc$role2)])
recipe2<-recipe1 |> add_steps_media(var_specific_controls=vc,media_controls=mc) |>  step_select(-has_role('postprocess'))
recipe3 <-recipe2  |># step_center(week) |>
update_role(c(sin1,sin2,sin3,sin4,sin5,cos1,cos2,cos3,cos4,cos5),new_role='time') |>
add_role(c(sin1,sin2,sin3,sin4,sin5,cos1,cos2,cos3,cos4,cos5),new_role='predictor') |>
step_novel(all_of(!!groupings)) |>
step_mutate_at(all_of(!!groupings),fn=list(id=as.integer))
if(get_control('tune_this_time',wc)=='FALSE'){
if(check_if_needs_tune(recipe3)){
if(get_control('saved_hypers_filename') =='' |
is.na(get_control('saved_hypers_filename') ) ){
stop("workflow controls calling for skipping tuning but the saved_hypers_filename control is empty!
Fill that control in with the location of saved hyperparameters")
}
recipe3<-recipe3|>tune::finalize_recipe(readRDS(get_control('saved_hypers_filename',wc)))
}
}
return(recipe3)
}
create_recipe(data1,vc=var_controls,mc=transform_controls,wc=workflow_controls)
# Keep eval=FALSE to avoid infinite loop in case you hit the knit button
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
# Keep eval=FALSE to avoid infinite loop in case you hit the knit button
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
pkgdown::build_site()
library(mostlytidyMMM)
suppressMessages(suppressWarnings(library(recipes)))
suppressMessages(suppressWarnings(library(tune)))
suppressMessages(suppressWarnings(library(dials)))
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(mostlytidyMMM)))
#get the control file:
control_file<-system.file('example model control.xlsx',package='mostlytidyMMM')
#get each relevant table of the control file:
var_controls<-readxl::read_xlsx(control_file,'variables')
transform_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow") |> select(-desc)
#pull data, add fourier transform columns
data1<-read.csv(system.file('example2.csv',package='mostlytidyMMM'))|>rename_columns_per_controls(variable_controls=var_controls)|>
rename_columns_per_controls()|> mutate(week=as.Date(week,"%m/%d/%Y"))|>
add_fourier_vars(vc=var_controls) |>  add_groups_and_sort(vc=var_controls)
#append several more columns
lotsa_vars<-paste0('x',1:60)
new_x<-replicate(60,runif(nrow(data1)))
names(new_x)<-lotsa_vars
new_x<-as_tibble(new_x)
#bring the data together:
data_for_lotsa_vars<-cbind(data1 ,new_x)
#make recipes
recipea<-recipe(head(data_for_lotsa_vars,n=1) )
recipeb<-recipea |> bulk_update_role() |> bulk_add_role()
recipeb<-recipeb |> add_steps_media() |>  step_select(-has_role('postprocess'))
recipec <-recipeb |> step_mutate(week=as.numeric(week)-19247.65) |>
update_role(c(sin1,sin2,sin3,cos1,cos2,cos3),new_role='time') |>
add_role(c(sin1,sin2,sin3,cos1,cos2,cos3),new_role='predictor')
recipec<-recipec |> update_role(starts_with('x'),new_role='predictor')
# build a model formula in the lmer style from the recipe:
(formula_with_lotsa_vars<-create_formula(recipec))
#build a list suitable for input to rethinking::ulam from the formula and recipe:
(create_ulam_list(model_formula=formula_with_lotsa_vars))
lotsa_vars
new_x
data_for_lotsa_vars
#make recipes
recipea<-recipe(head(data_for_lotsa_vars) )
recipeb<-recipea |> bulk_update_role() |> bulk_add_role()
recipeb<-recipeb |> add_steps_media() |>  step_select(-has_role('postprocess'))
recipec <-recipeb |> step_mutate(week=as.numeric(week)-19247.65) |>
update_role(c(sin1,sin2,sin3,cos1,cos2,cos3),new_role='time') |>
add_role(c(sin1,sin2,sin3,cos1,cos2,cos3),new_role='predictor')
recipec<-recipec |> update_role(starts_with('x'),new_role='predictor')
recipec<-recipec |> update_role(starts_with('V'),new_role='predictor')
# build a model formula in the lmer style from the recipe:
(formula_with_lotsa_vars<-create_formula(recipec))
# Keep eval=FALSE to avoid infinite loop in case you hit the knit button
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
# Keep eval=FALSE to avoid infinite loop in case you hit the knit button
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(mostlytidyMMM)
#not run!
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(recipes)))
suppressMessages(suppressWarnings(library(tune)))
suppressMessages(suppressWarnings(library(dials)))
suppressMessages(suppressWarnings(library(mostlytidyMMM)))
#create two datasets
mktdata<-rbind(tibble(prod='brand',store='store1',
sales=c(100,100,100,100,100),tv=c(10,100,0,0,100),
search=c(0,10,20,50,50)) ,
tibble(prod='brand',store='store2',
sales=c(10,10,10,10,10),tv=c(0,0,0,0,0),
search=c(0,2,2,0,0) ) ) |>
group_by(prod,store)
mktdata2<-tibble(prod='brand',store='all',
sales=100,tv=1000,search=1000) |> group_by(prod,store)
#creating a recipe with a saturaiton step in it
rec_obj <-
recipe(sales ~ ., data = mktdata) |>
step_saturation(c(tv,search),asymptote=200,saturation_speed=.003 ) |>
prep(training = mktdata)
#not run!
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(recipes)))
suppressMessages(suppressWarnings(library(tune)))
suppressMessages(suppressWarnings(library(dials)))
suppressMessages(suppressWarnings(library(mostlytidyMMM)))
#create two datasets
mktdata<-rbind(tibble(prod='brand',store='store1',
sales=c(100,100,100,100,100),tv=c(10,100,0,0,100),
search=c(0,10,20,50,50)) ,
tibble(prod='brand',store='store2',
sales=c(10,10,10,10,10),tv=c(0,0,0,0,0),
search=c(0,2,2,0,0) ) ) |>
group_by(prod,store)
mktdata2<-tibble(prod='brand',store='all',
sales=100,tv=1000,search=1000) |> group_by(prod,store)
#creating a recipe with a saturaiton step in it
rec_obj <-
recipe(sales ~ ., data = mktdata) |>
step_saturation(c(tv,search),asymptote=200,saturation_speed=.003 ) |>
prep(training = mktdata)
rec_obj <-
recipe(sales ~ ., data = mktdata)
recipe(sales ~ ., data = mktdata) |>
step_saturation(c(tv,search),asymptote=200,saturation_speed=.003 )
#creating a recipe with a saturaiton step in it
rec_obj <-
recipe(sales ~ ., data = mktdata) |> step_saturation(c(tv,search),asymptote=200,saturation_speed=.003 ) |>prep(training = mktdata)
rec_obj <-
recipe(sales ~ ., data = mktdata) |> step_saturation(c(tv,search),asymptote=200,saturation_speed=.003 )
rec_obj
rec_obj %>% prep(training=mktdata)
#' @param x an object (step or recipe)
#' @param training  tibble used to provide data for training
#'
#' @return
#' a prepped recipe or step
#' @export
#' @export prep.step_saturation
#'
#' @importFrom recipes prep
#'
prep.step_saturation <- function(x, training, info = NULL, ...) {
#this will select the appropriate columns from the training set
col_names <- recipes_eval_select(x$terms, training, info)
## We'll use the names later so make sure they are available
if (x$options$names == FALSE) {
rlang::abort("`names` should be set to TRUE")
}
#transforms computed here
step_saturation_new(terms=x$terms,
trained=TRUE,
role=x$role,
options=x$options,
skip=x$skip,
id=x$id,
asymptote=x$asymptote,
saturation_speed=x$saturation_speed,
hills=col_names
)
}
rec_obj %>% prep(training=mktdata)
library(mostlytidyMMM)
rec_obj %>% prep(training=mktdata)
# Keep eval=FALSE to avoid infinite loop in case you hit the knit button
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
# Keep eval=FALSE to avoid infinite loop in case you hit the knit button
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
# Keep eval=FALSE to avoid infinite loop in case you hit the knit button
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
# Chunk 1: development-load
# Load already included functions if relevant
pkgload::load_all(export_all = FALSE)
# Chunk 2: start
suppressMessages(suppressWarnings(library(recipes)))
suppressMessages(suppressWarnings(library(tune)))
suppressMessages(suppressWarnings(library(dials)))
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(workflowsets)))
suppressMessages(suppressWarnings(library(tidymodels)))
suppressMessages(suppressWarnings(library(multilevelmod)))
suppressMessages(suppressWarnings(library(rethinking)))
suppressMessages(suppressWarnings(library(mostlytidyMMM)))
#get the control file:
control_file<-system.file('example model control.xlsx',package='mostlytidyMMM')
#get each relevant table of the control file:
variables<-readxl::read_xlsx(control_file,'variables')
role_controls<-readxl::read_xlsx(control_file,'role controls')
workflow_controls<-readxl::read_xlsx(control_file,"workflow")
# Chunk 3: examples-dev_hyperparameterand_formula_search_rmd
variables
# Chunk 4: more
role_controls
# Chunk 5: more1
workflow_controls
# Chunk 6: dataingest
data1<-read.csv(system.file('example2.csv',package='mostlytidyMMM'))|>
rename_columns_per_controls(variable_controls=variables)|> mutate(week=as.Date(week,"%m/%d/%Y"))|>
add_fourier_vars(vc=variables) |>  add_groups_and_sort(vc=variables)
# Chunk 7: recipe
recipe3<-create_recipe(data_to_use=data1,
vc=variables,
mc=role_controls,
wc=workflow_controls)
# Chunk 8: proovetune
recipe3
check_if_needs_tune(recipe3)
# Chunk 9: produce-workflow-set-first-ass
fft_formulae0<-make_list_of_fft_formulae(workflow_controls,recipe3)
#split the output into two lists, one for formula and one for workflow config tables:
#list must have names for workflowsets
formulae<-fft_formulae0[[1]]
names(formulae)<-as.character(1:length(formulae))
configs_fft_options<-fft_formulae0[[2]]
length(formulae)
formulae[[1]]
formulae[[2]]
formulae[[3]]
# Chunk 10: assemble
list_of_flows<-lapply(formulae,assemble_workflow,recipe3)
tune_all_these<-as_workflow_set(!!!list_of_flows)
time_id_var = variables |>filter(role=='time_id')|>select(varname)|>unlist()
#noe use of sliding_period for 'resampling'
data_splits<-sliding_period(data1 |>ungroup()|> arrange(across(all_of(!!time_id_var))),!!time_id_var,period='week',
lookback=90,assess_stop=4,step=5)
#note that we can't say we are doing a good job detecting seasonality with 100 week training periods . .
fft_selecting_tune_results<-workflow_map(tune_all_these,grid=5,resamples=data_splits)
# Chunk 11: autoplot
autoplot(fft_selecting_tune_results,metric='rmse')
# Chunk 12: pickbest
id_of_best<-rank_results(fft_selecting_tune_results,rank_metric="rmse",select_best=F) |>
select(wflow_id) %>% slice_head(n=1) |> unlist()
if(check_if_needs_tune(recipe3)){
hyper_parms<-select_best(
extract_workflow_set_result(fft_selecting_tune_results,id=id_of_best),metric='rmse')
hyper_parms_finalized_recipe<-recipe3 %>% finalize_recipe(hyper_parms)
}else{
hyper_parms_finalized_recipe<-recipe3
}
best_seas_vc<-configs_fft_options[[as.numeric(id_of_best)]]
best_seas_formula<-formulae[[as.numeric(id_of_best)]]
# Chunk 13: searchrandom
(list_of_formulae_rands<-make_list_of_rands_formula(
seasonality_formula = best_seas_formula,
vc = best_seas_vc
) )
list_of_flows2<-lapply(list_of_formulae_rands,assemble_workflow,hyper_parms_finalized_recipe)
names(list_of_flows2)<-as.character(1:length(list_of_formulae_rands))
tune_all_these2<-as_workflow_set(!!!list_of_flows2)
data_splits<-sliding_period(data1 |>ungroup()|> arrange(across(all_of(!!time_id_var))),!!time_id_var,period='week',
lookback=90,assess_stop=4,step=5)
rands_selecting_tune<-workflow_map(tune_all_these2,grid=1,resamples=data_splits)
rank_results(rands_selecting_tune)
id_of_best_rand<-rank_results(rands_selecting_tune,rank_metric="rmse",select_best=T) %>%
select(wflow_id) %>% slice_head(n=1) %>% unlist()
best_formula<-list_of_formulae_rands[[as.numeric(id_of_best_rand)]][1]
# Chunk 14
hyper_parms
best_formula
# Chunk 15
boundaries<-make_bound_statements(variable_controls=variables)
formula_list_for_final<-create_ulam_list(prior_controls=variables, model_formula=best_formula)
data_to_model<-bake(hyper_parms_finalized_recipe|>prep(),data1)
final_fit_model<-rethinking::ulam(formula_list_for_final,
constraints = boundaries,
chains=1,iter=100,
data=data_to_model,
sample = T,
cmdstan = T,
#file='final_fit_model',
cores=4,
declare_all_data=F,
messages=F
)
# Chunk 16: example6
data_to_model$pred<-predict.ulam(final_fit_model,data_to_model)[,1]
# Chunk 17: example7
this_rsq<-rsq(data_to_model|>ungroup(),truth=sales,estimate=pred)['.estimate'] %>% unlist()
this_mape<-mape(data_to_model|>ungroup(),truth=sales,estimate=pred)['.estimate'] %>% unlist()
ggplot(data_to_model ,aes(x=sales,y=pred,color=store_id))+
geom_point()+ geom_abline(slope=1,intercept=0)+ggthemes::theme_tufte()+
ggtitle("Predicted vs Actual",subtitle=paste0('Rsq is ',round(this_rsq,2)))
# Chunk 18: example8
model_preds_long<-data_to_model %>% pivot_longer(c(pred,sales))
ggplot(model_preds_long,aes(x=week,y=value,color=name))+geom_line()+
ggtitle("Sales and Predicted Sales by Week",subtitle=paste('MAPE is',round(this_mape)))
# Chunk 19: example9
decomps<-get_decomps_irregardless(data_to_model %>% ungroup(),recipe_to_use=hyper_parms_finalized_recipe,
model_obj=final_fit_model,
)
# Chunk 20: example10
decomps_natl<-decomps %>% select(week,all_of(!!get_predictors_vector(hyper_parms_finalized_recipe))) %>% group_by(week) %>% summarise(across(where(is.numeric),sum))
decomps_natl<-decomps_natl %>% pivot_longer(cols=c(-week))
ggplot(data=decomps_natl,aes(x=week,y=value,fill=name)) + geom_area()+ggthemes::theme_tufte()+
ggtitle("Decomposition By Week")+
theme(legend.position = 'bottom')
# Run but keep eval=FALSE to avoid infinite loop
# Execute in the console directly
fusen::inflate(flat_file = "dev/flat_dev-hyperparameterand-formula-search.Rmd", vignette_name = "Hyperparameter and Formula Search")
pkgdown::build_site()
?fusen::add_additional
fusen::add_additional(flat_name='introducing_control_file')
library(testthat)
# Load already included functions if relevant
pkgload::load_all(export_all = FALSE)
control_file<-system.file('example model control.xlsx',package='mostlytidyMMM')
View(control_file)
write_file(control_file,'example.xlsx')
?file.copy
file.copy(control_file,'example.xlsx')
file.copy(control_file,'example.xlsx')
file.copy(control_file,'example.xlsx',overwrite=T)
variables<-readxl::read_xlsx(control_file,'variables')
head(variables)
unique(variables$role)
unique(variables$role2)
fusen::add_additional(flat_name="control_file_workflow",overwrite=T)
library(mostlyTidyMMM)
library(mostlytidyMMM)
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
librarian::shelf('multilevelmod')
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
fusen::inflate(flat_file = "dev/flat_full.Rmd", vignette_name = "Demonstration of Parts of the Package")
